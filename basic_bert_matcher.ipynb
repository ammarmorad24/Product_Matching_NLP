{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10669432,"sourceType":"datasetVersion","datasetId":6608149},{"sourceId":10671831,"sourceType":"datasetVersion","datasetId":6609972},{"sourceId":10673631,"sourceType":"datasetVersion","datasetId":6611300}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"b7f0261a-755f-44ce-b0f5-19062b8b9e03","_cell_guid":"16a63aad-47a5-4779-bcac-c1970d86a73c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-06T20:15:33.778329Z","iopub.execute_input":"2025-02-06T20:15:33.778722Z","iopub.status.idle":"2025-02-06T20:15:33.809462Z","shell.execute_reply.started":"2025-02-06T20:15:33.778693Z","shell.execute_reply":"2025-02-06T20:15:33.808369Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/product-matching-dataset/Masterfile.xlsx\n/kaggle/input/product-matching-dataset/Dataset.xlsx\n/kaggle/input/test-set-data/augmented_test_set.xlsx\n/kaggle/input/test-data/filtered_masterfile (1).xlsx\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Install required libraries\n!pip install sentence-transformers pandas openpyxl faiss-cpu gensim nltk fuzzywuzzy python-Levenshtein pyjarowinkler\n\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport faiss\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\nimport re\nimport Levenshtein\nfrom fuzzywuzzy import fuzz\nfrom pyjarowinkler.distance import get_jaro_distance\nfrom nltk.tokenize import word_tokenize\n\n# Arabic Normalization Function\ndef normalize_arabic(text):\n    text = re.sub(r\"[إأآا]\", \"ا\", text)\n    text = re.sub(r\"[يى]\", \"ي\", text)\n    text = re.sub(r\"ة\", \"ه\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# Fuzzy Matching Functions\ndef levenshtein_similarity(s1, s2):\n    return 1 - (Levenshtein.distance(s1, s2) / max(len(s1), len(s2)))\n\ndef jaro_winkler_similarity(s1, s2):\n    return get_jaro_distance(s1, s2)\n\ndef fuzzy_wuzzy_similarity(s1, s2):\n    return fuzz.token_sort_ratio(s1, s2) / 100.0\n\n# Semantic Matching using BERT\ndef encode_texts(model, texts):\n    return np.array(model.encode(texts, convert_to_numpy=True))\n\n# Precompute Master Embeddings for Fast Lookups\ndef build_faiss_index(embeddings):\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings)\n    return index\n\n# Function to match dataset with master file using FAISS for Fast Search\ndef match_dataset(master_file, dataset_file, output_file, model):\n    master_df = pd.read_excel(master_file)\n    dataset_df = pd.read_excel(dataset_file)\n    \n    master_df['Normalized Name'] = master_df['product_name_ar'].apply(normalize_arabic)\n    dataset_df['Normalized Name'] = dataset_df['product_name_ar'].apply(normalize_arabic)\n    \n    master_embeddings = encode_texts(model, master_df['Normalized Name'].tolist())\n    index = build_faiss_index(master_embeddings)\n    \n    dataset_embeddings = encode_texts(model, dataset_df['Normalized Name'].tolist())\n    \n    D, I = index.search(dataset_embeddings, 1)  # Find closest match\n    \n    results = []\n    for i, row in dataset_df.iterrows():\n        best_match_idx = I[i][0]\n        best_match = master_df.iloc[best_match_idx]['product_name_ar']\n        best_sku = master_df.iloc[best_match_idx]['sku']\n        best_score = 1 / (1 + D[i][0])  # Convert distance to similarity\n        \n        results.append({\n            'Item code': row['sku'],\n            'product_name_ar': row['product_name_ar'],\n            'Matched Item': best_match,\n            'sku': best_sku,\n            'Similarity Score': best_score\n        })\n    \n    result_df = pd.DataFrame(results)\n    result_df.to_excel(output_file, index=False)\n    print(f\"Matching completed. Output saved to {output_file}\")\n\n# Example Usage\nif __name__ == \"__main__\":\n    try:\n        model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")  # Arabic-compatible BERT\n    except Exception as e:\n        print(f\"Failed to load BERT model: {e}\")\n        model = None  # Fallback if model fails\n    \n    master_file = \"/kaggle/input/product-matching-dataset/Masterfile.xlsx\"\n    dataset_file = \"/kaggle/input/test-set-data/augmented_test_set.xlsx\"\n    output_file = \"/kaggle/working/matched_results.xlsx\"\n    \n    match_dataset(master_file, dataset_file, output_file, model)","metadata":{"_uuid":"6706754c-9a9d-4d11-b5fd-16fac231ae93","_cell_guid":"f9cc7c64-5a10-4c53-b66a-ff9dc57b5e72","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T20:15:33.810898Z","iopub.execute_input":"2025-02-06T20:15:33.811315Z","iopub.status.idle":"2025-02-06T20:15:51.402001Z","shell.execute_reply.started":"2025-02-06T20:15:33.811273Z","shell.execute_reply":"2025-02-06T20:15:51.400755Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\nRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.10.0)\nRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\nRequirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\nRequirement already satisfied: pyjarowinkler in /usr/local/lib/python3.10/dist-packages (2.0.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.1)\nRequirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.12.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649079fc742443cc8c99749f36cc75c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b1635bcfd04b9699868e77f4656358"}},"metadata":{}},{"name":"stdout","text":"Matching completed. Output saved to /kaggle/working/matched_results.xlsx\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import openpyxl\n\ndef calculate_sku_ratio(xlsx_filepath):\n    \"\"\"\n    Calculates the ratio of equal \"Item Code\" and \"SKU\" values to the total number of items.\n\n    Args:\n        xlsx_filepath: The path to the XLSX file.\n\n    Returns:\n        A tuple containing:\n            - The ratio of equal SKUs (a float between 0 and 1).\n            - A list of error messages, if any.\n    \"\"\"\n\n    try:\n        workbook = openpyxl.load_workbook(xlsx_filepath)\n        sheet = workbook.active\n\n        item_code_column = None\n        sku_column = None\n        header_row = None\n\n        for row in sheet.iter_rows(max_row=1):  # Check only the first row for headers\n            header_row = [cell.value for cell in row]\n            for i, cell_value in enumerate(header_row):\n                if cell_value is not None:  # Handle potential None values in headers\n                    if \"item code\" in cell_value.lower():\n                        item_code_column = i + 1\n                    if \"sku\" in cell_value.lower():\n                        sku_column = i + 1\n            if item_code_column is not None and sku_column is not None:\n                break\n\n        if item_code_column is None or sku_column is None:\n            return 0.0, [\"'item code' or 'sku' column not found (case-insensitive) in the first row.\"]\n\n        equal_sku_count = 0\n        total_items = 0\n        errors = []\n\n        for row_num in range(2, sheet.max_row + 1):\n            total_items += 1  # Increment total items for each row (excluding header)\n            item_code_cell = sheet.cell(row=row_num, column=item_code_column)\n            sku_cell = sheet.cell(row=row_num, column=sku_column)\n\n            item_code = item_code_cell.value\n            sku = sku_cell.value\n\n            if item_code is None and sku is None:  # If both are None, consider it a match\n                equal_sku_count += 1\n                continue\n            elif item_code is None or sku is None:  # If only one is None, it is not a match\n                continue\n\n\n            try:\n                item_code = str(item_code)\n                sku = str(sku)\n            except Exception as e:\n                errors.append(f\"Error converting values in row {row_num}: {e}\")\n                continue  # Skip this row if there's a conversion error\n\n            if item_code.strip().lower() == sku.strip().lower():\n                equal_sku_count += 1\n\n        ratio = 0.0 if total_items == 0 else equal_sku_count / total_items  # Handle division by zero\n        return ratio, errors\n\n    except FileNotFoundError:\n        return 0.0, [\"File not found.\"]\n    except Exception as e:\n        return 0.0, [f\"An error occurred: {e}\"]\n\n\n\n# Example usage:\nfilepath = \"/kaggle/working/matched_results.xlsx\"  # Replace with your file path\nratio, errors = calculate_sku_ratio(filepath)\n\nif errors:\n    for err in errors:\n        print(err)\nelse:\n    print(f\"Ratio of equal SKUs: {ratio}\")","metadata":{"_uuid":"518cf8df-5688-43c6-8450-e54890410baf","_cell_guid":"f0e1bf0e-c16f-4be7-ab48-6643bbb9debc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-06T20:15:51.403936Z","iopub.execute_input":"2025-02-06T20:15:51.404243Z","iopub.status.idle":"2025-02-06T20:15:51.475902Z","shell.execute_reply.started":"2025-02-06T20:15:51.404215Z","shell.execute_reply":"2025-02-06T20:15:51.474794Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Ratio of equal SKUs: 0.8376237623762376\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"_uuid":"350c57f8-993e-4f31-8211-09d95727e25b","_cell_guid":"d96ddd23-fc7f-4698-921a-ae93fabd76ea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}