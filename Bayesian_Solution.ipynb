{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"./Masterfile.xlsx\"\n",
    "training = \"./Dedup_Dataset.xlsx\"\n",
    "test = \"./test.xlsx\"\n",
    "\n",
    "# dynamic test column names\n",
    "ITEM_NAME = \"product name\"\n",
    "ITEM_CODE = \"item code\"\n",
    "ITEM_PRICE = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    \"\"\"Harsh and Kinda extreme preprocessing but for the clarity\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#\\'()*+,.:;<=>?@[\\\\]^_`{|}~]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)  # Remove specific words and everything after them\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)  # Remove specific words even if they are part of another word\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)  # Remove repeated letters\n",
    "    return text\n",
    "\n",
    "\n",
    "def optimize_normalize_arabic(text):\n",
    "    \"\"\"Enhanced Arabic text normalization that preserves % and / characters\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(araby.normalize_ligature(araby.normalize_alef(araby.normalize_teh(text))))\n",
    "    \n",
    "    # Replace unwanted characters except % and /\n",
    "    text = re.sub(r'[^\\w\\s%/]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove specific Arabic terms and everything after them\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)\n",
    "    \n",
    "    # Remove specific terms even if they're part of another word\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)\n",
    "    \n",
    "    # Remove repeated letters\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian model training...\n",
      "Preparing training data...\n",
      "Reading data files...\n",
      "Training samples: 501\n",
      "Master products: 1000\n",
      "Found 500 matching SKUs\n",
      "Normalizing text...\n",
      "Initializing TF-IDF vectorizer...\n",
      "Creating training pairs...\n",
      "Processing query 0/500\n",
      "Processing query 43400/500\n",
      "Processing query 44600/500\n",
      "Processing query 46600/500\n",
      "Computing features for all pairs...\n",
      "Total pairs: 3000\n",
      "Positive pairs: 500\n",
      "Negative pairs: 2500\n",
      "Splitting data...\n",
      "Training Gaussian Naive Bayes model...\n",
      "Train accuracy: 0.9404\n",
      "Test accuracy: 0.9550\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       500\n",
      "           1       0.83      0.91      0.87       100\n",
      "\n",
      "    accuracy                           0.95       600\n",
      "   macro avg       0.91      0.94      0.92       600\n",
      "weighted avg       0.96      0.95      0.96       600\n",
      "\n",
      "Model saved as bayesian_product_matcher.pkl\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "def compute_optimized_features_batch(pairs, tfidf_vectorizer):\n",
    "    \"\"\"Compute features for multiple pairs at once with vectorization\"\"\"\n",
    "    features = np.zeros((len(pairs), 4))\n",
    "    \n",
    "    texts1, texts2 = zip(*pairs)\n",
    "    \n",
    "    tfidf_vectors1 = tfidf_vectorizer.transform(texts1)\n",
    "    tfidf_vectors2 = tfidf_vectorizer.transform(texts2)\n",
    "    \n",
    "    cosine_sims = cosine_similarity(tfidf_vectors1, tfidf_vectors2)\n",
    "    \n",
    "    for i, (text1, text2) in enumerate(pairs):\n",
    "        features[i, 0] = fuzz.ratio(text1, text2)\n",
    "        features[i, 1] = fuzz.token_set_ratio(text1, text2)\n",
    "        \n",
    "        tokens1 = set(text1.split())\n",
    "        tokens2 = set(text2.split())\n",
    "        features[i, 2] = len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 or tokens2 else 0.0\n",
    "        features[i, 3] = cosine_sims[i, i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_bayesian_training_data(train_file, master_file, neg_samples_per_query=5):\n",
    "    \"\"\"\n",
    "    Prepare training data for Bayesian classification approach with robust data validation\n",
    "    \"\"\"\n",
    "    print(\"Reading data files...\")\n",
    "    train_df = pd.read_excel(train_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file, usecols=['sku', 'product_name_ar'])\n",
    "    \n",
    "    # Remove any duplicates and null values\n",
    "    train_df = train_df.dropna(subset=['sku', 'seller_item_name']).drop_duplicates(subset=['sku'])\n",
    "    master_df = master_df.dropna(subset=['sku', 'product_name_ar']).drop_duplicates(subset=['sku'])\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Master products: {len(master_df)}\")\n",
    "    \n",
    "    # Find matching SKUs between training and master data\n",
    "    matching_skus = set(train_df['sku']).intersection(set(master_df['sku']))\n",
    "    print(f\"Found {len(matching_skus)} matching SKUs\")\n",
    "    \n",
    "    if len(matching_skus) == 0:\n",
    "        raise ValueError(\"No matching SKUs found between training and master data\")\n",
    "    \n",
    "    # Filter to only matching SKUs\n",
    "    train_df = train_df[train_df['sku'].isin(matching_skus)]\n",
    "    \n",
    "    print(\"Normalizing text...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, train_df['seller_item_name']\n",
    "        ))\n",
    "        master_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, master_df['product_name_ar']\n",
    "        ))\n",
    "    \n",
    "    print(\"Initializing TF-IDF vectorizer...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=2,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    all_texts = pd.concat([train_df['Normalized_Product'], master_df['Normalized_Product']]).unique()\n",
    "    tfidf_vectorizer.fit(all_texts)\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "        pickle.dump(tfidf_vectorizer, file)\n",
    "    \n",
    "    print(\"Creating training pairs...\")\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Create a master product lookup dictionary for efficiency\n",
    "    master_lookup = master_df.set_index('sku')['Normalized_Product'].to_dict()\n",
    "    \n",
    "    for idx, query_row in train_df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing query {idx}/{len(train_df)}\")\n",
    "            \n",
    "        query_sku = query_row['sku']\n",
    "        query_text = query_row['Normalized_Product']\n",
    "        \n",
    "        # Get positive match from lookup dictionary\n",
    "        positive_match = master_lookup[query_sku]\n",
    "        \n",
    "        # Add positive pair\n",
    "        pairs.append((query_text, positive_match))\n",
    "        labels.append(1)\n",
    "        \n",
    "        # Sample negative matches from products with different SKUs\n",
    "        negative_skus = np.random.choice(\n",
    "            [sku for sku in master_lookup.keys() if sku != query_sku],\n",
    "            size=min(neg_samples_per_query, len(master_lookup)-1),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for neg_sku in negative_skus:\n",
    "            negative_match = master_lookup[neg_sku]\n",
    "            pairs.append((query_text, negative_match))\n",
    "            labels.append(0)\n",
    "    \n",
    "    if not pairs:\n",
    "        raise ValueError(\"No valid training pairs could be created\")\n",
    "    \n",
    "    # Compute features for all pairs\n",
    "    print(\"Computing features for all pairs...\")\n",
    "    X = compute_optimized_features_batch(pairs, tfidf_vectorizer)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"Total pairs: {len(X)}\")\n",
    "    print(f\"Positive pairs: {sum(y)}\")\n",
    "    print(f\"Negative pairs: {len(y) - sum(y)}\")\n",
    "    \n",
    "    return X, y, tfidf_vectorizer\n",
    "\n",
    "def train_bayesian_model(train_file, master_file):\n",
    "    \"\"\"Train a Gaussian Naive Bayes model for product matching\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, tfidf_vectorizer = prepare_bayesian_training_data(train_file, master_file)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(\"Training Gaussian Naive Bayes model...\")\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model\n",
    "    with open(\"bayesian_product_matcher.pkl\", 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(\"Model saved as bayesian_product_matcher.pkl\")\n",
    "    \n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting Bayesian model training...\")\n",
    "model, vectorizer = train_bayesian_model(training, master)\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect if text is primarily Arabic or English based on character count\n",
    "    Returns 'arabic' if primarily Arabic, 'english' if primarily English\n",
    "    \"\"\"\n",
    "    # Count English letters (a-z, A-Z)\n",
    "    english_count = len(re.findall(r'[a-zA-Z]', text))\n",
    "    \n",
    "    # If more than 3 English letters, consider it English\n",
    "    if english_count > 3:\n",
    "        return 'english'\n",
    "    return 'arabic'\n",
    "\n",
    "def normalize_english(text):\n",
    "    \"\"\"Normalize English text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common product-related words and everything after them\n",
    "    text = re.sub(r'\\b(?:price|new|old|p n|p o)\\b.*', '', text)\n",
    "    \n",
    "    # Remove repeated letters (e.g., 'goood' -> 'good')\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def fast_normalize_arabic(text):\n",
    "    \"\"\"Extremely fast Arabic text normalization focusing only on critical operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#\\'()*+,.:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text based on detected language\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    language = detect_language(text)\n",
    "    if language == 'english':\n",
    "        return normalize_english(text)\n",
    "    return fast_normalize_arabic(text)\n",
    "\n",
    "def compute_token_overlap(text1, text2):\n",
    "    \"\"\"Compute token overlap between two texts\"\"\"\n",
    "    tokens1 = set(text1.split())\n",
    "    tokens2 = set(text2.split())\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    return len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "\n",
    "def precompute_tfidf_matrix(texts, vectorizer):\n",
    "    \"\"\"Precompute TF-IDF matrix for all texts\"\"\"\n",
    "    return vectorizer.transform(texts)\n",
    "\n",
    "def compute_batch_features(query_texts, master_texts, query_tfidf, master_tfidf):\n",
    "    \"\"\"Compute features for a batch of text pairs efficiently\"\"\"\n",
    "    # Calculate cosine similarities for the entire batch at once\n",
    "    cosine_sims = cosine_similarity(query_tfidf, master_tfidf)\n",
    "    \n",
    "    n_queries = len(query_texts)\n",
    "    n_masters = len(master_texts)\n",
    "    \n",
    "    # Initialize feature matrices\n",
    "    fuzz_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_set_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_overlaps = np.zeros((n_queries, n_masters))\n",
    "    \n",
    "    # Compute features in parallel\n",
    "    def compute_pair_features(i, j):\n",
    "        return (\n",
    "            fuzz.ratio(query_texts[i], master_texts[j]),\n",
    "            fuzz.token_set_ratio(query_texts[i], master_texts[j]),\n",
    "            compute_token_overlap(query_texts[i], master_texts[j])\n",
    "        )\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i in range(n_queries):\n",
    "            for j in range(n_masters):\n",
    "                futures.append(executor.submit(compute_pair_features, i, j))\n",
    "        \n",
    "        for idx, future in enumerate(futures):\n",
    "            i = idx // n_masters\n",
    "            j = idx % n_masters\n",
    "            ratio, token_set, overlap = future.result()\n",
    "            fuzz_ratios[i, j] = ratio\n",
    "            token_set_ratios[i, j] = token_set\n",
    "            token_overlaps[i, j] = overlap\n",
    "    \n",
    "    features = np.stack([\n",
    "        fuzz_ratios,\n",
    "        token_set_ratios,\n",
    "        token_overlaps,\n",
    "        cosine_sims\n",
    "    ], axis=2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def match_products_bilingual(query_file, master_file, bayesian_model, vectorizer, threshold=0.5, output_file=\"MatchedResults_Pairwise.xlsx\"):\n",
    "    \"\"\"Bilingual matching function supporting both Arabic and English text\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    \n",
    "    # Load data efficiently\n",
    "    query_df = pd.read_excel(query_file, usecols=[ITEM_CODE, ITEM_NAME, ITEM_PRICE])\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    \n",
    "    print(f\"Loaded {len(query_df)} query products and {len(master_df)} master products\")\n",
    "    \n",
    "    # Detect language for each query product\n",
    "    print(\"Detecting languages and normalizing texts...\")\n",
    "    query_df['detected_language'] = query_df[ITEM_NAME].apply(detect_language)\n",
    "    query_df['Normalized_Product'] = query_df[ITEM_NAME].apply(normalize_text)\n",
    "    \n",
    "    # Initialize master product columns\n",
    "    master_df['Normalized_Product_AR'] = master_df['product_name_ar'].apply(fast_normalize_arabic)\n",
    "    master_df['Normalized_Product_EN'] = master_df['product_name'].apply(normalize_english)\n",
    "    \n",
    "    # Process Arabic and English queries separately\n",
    "    results = []\n",
    "    \n",
    "    # Handle Arabic queries\n",
    "    arabic_queries = query_df[query_df['detected_language'] == 'arabic']\n",
    "    if len(arabic_queries) > 0:\n",
    "        print(\"Processing Arabic queries...\")\n",
    "        arabic_query_tfidf = precompute_tfidf_matrix(arabic_queries['Normalized_Product'], vectorizer)\n",
    "        arabic_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_AR'], vectorizer)\n",
    "        \n",
    "        arabic_features = compute_batch_features(\n",
    "            arabic_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_AR'].tolist(),\n",
    "            arabic_query_tfidf,\n",
    "            arabic_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process Arabic predictions\n",
    "        arabic_predictions = process_predictions(\n",
    "            arabic_queries,\n",
    "            master_df,\n",
    "            arabic_features,\n",
    "            bayesian_model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(arabic_predictions)\n",
    "    \n",
    "    # Handle English queries\n",
    "    english_queries = query_df[query_df['detected_language'] == 'english']\n",
    "    if len(english_queries) > 0:\n",
    "        print(\"Processing English queries...\")\n",
    "        english_query_tfidf = precompute_tfidf_matrix(english_queries['Normalized_Product'], vectorizer)\n",
    "        english_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_EN'], vectorizer)\n",
    "        \n",
    "        english_features = compute_batch_features(\n",
    "            english_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_EN'].tolist(),\n",
    "            english_query_tfidf,\n",
    "            english_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process English predictions\n",
    "        english_predictions = process_predictions(\n",
    "            english_queries,\n",
    "            master_df,\n",
    "            english_features,\n",
    "            bayesian_model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(english_predictions)\n",
    "    \n",
    "    # Create and save results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"Matching completed in {processing_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(processing_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def process_predictions(query_df, master_df, features, bayesian_model, threshold):\n",
    "    \"\"\"\n",
    "    Process predictions for a set of queries using Naive Bayes model\n",
    "    \n",
    "    Parameters:\n",
    "    - query_df: DataFrame containing query products\n",
    "    - master_df: DataFrame containing master products\n",
    "    - features: Computed similarity features\n",
    "    - bayesian_model: GaussianNB model\n",
    "    - threshold: Minimum probability threshold for accepting matches\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing match results\n",
    "    \"\"\"\n",
    "    n_queries, n_masters, n_features = features.shape\n",
    "    features_reshaped = features.reshape(-1, n_features)\n",
    "    \n",
    "    # Get prediction probabilities from the Bayesian model\n",
    "    predictions = bayesian_model.predict_proba(features_reshaped)\n",
    "    \n",
    "    # Extract probabilities for the positive class (class 1)\n",
    "    if predictions.shape[1] >= 2:  # For multi-class case\n",
    "        predictions = predictions[:, 1]  # Take the probability of class 1\n",
    "    else:  # For binary case with only one probability output\n",
    "        predictions = np.ones(len(predictions))  # Default to 1 if only one class\n",
    "\n",
    "    predictions = predictions.reshape(n_queries, n_masters)\n",
    "    \n",
    "    best_match_indices = np.argmax(predictions, axis=1)\n",
    "    best_match_scores = np.max(predictions, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    for i, (_, query_row) in enumerate(query_df.iterrows()):\n",
    "        if best_match_scores[i] >= threshold:\n",
    "            master_row = master_df.iloc[best_match_indices[i]]\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row[ITEM_CODE],\n",
    "                \"Query Product\": query_row[ITEM_NAME],\n",
    "                \"Price\": query_row[ITEM_PRICE],\n",
    "                \"Matched Master SKU\": master_row['sku'],\n",
    "                \"Matched Master Product\": master_row['product_name_ar'] if query_row['detected_language'] == 'arabic' else master_row['product_name'],\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row[ITEM_CODE],\n",
    "                \"Query Product\": query_row[ITEM_NAME],\n",
    "                \"Price\": query_row[ITEM_PRICE],\n",
    "                \"Matched Master SKU\": None,\n",
    "                \"Matched Master Product\": None,\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_matching_with_timing(query_file, master_file, bayesian_model, vectorizer, output_file=\"MatchedResults.xlsx\"):\n",
    "    \"\"\"Run matching with detailed timing information\"\"\"\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = match_products_bilingual(query_file, master_file, bayesian_model, vectorizer, output_file=output_file)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(total_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian model loaded from file successfully.\n",
      "Running the matching process with Bayesian model...\n",
      "Starting bilingual matching process...\n",
      "Starting bilingual matching process...\n",
      "Loaded 72 query products and 1000 master products\n",
      "Detecting languages and normalizing texts...\n",
      "Processing Arabic queries...\n",
      "Processing English queries...\n",
      "Matching completed in 3.68 seconds\n",
      "Average time per product: 51.07 ms\n",
      "\n",
      "Performance Summary:\n",
      "Total processing time: 3.68 seconds\n",
      "Average time per product: 51.07 ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the Bayesian model you trained earlier\n",
    "# You can use the model variable that already exists or reload from the saved file\n",
    "model_from_file = None\n",
    "try:\n",
    "    with open(\"bayesian_product_matcher.pkl\", 'rb') as file:\n",
    "        model_from_file = pickle.load(file)\n",
    "    print(\"Bayesian model loaded from file successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load model from file: {e}\")\n",
    "    print(\"Using the model from memory instead.\")\n",
    "\n",
    "# Use the loaded model or the one in memory\n",
    "bayesian_model = model_from_file if model_from_file is not None else model\n",
    "\n",
    "print(\"Running the matching process with Bayesian model...\")\n",
    "results = run_matching_with_timing(\n",
    "    test,\n",
    "    master,\n",
    "    bayesian_model,\n",
    "    vectorizer,\n",
    "    output_file=\"MatchedResults_Bayesian.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Product_Matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
