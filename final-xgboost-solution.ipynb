{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model and Code Requirements\n",
    "%pip install pandas numpy scikit-learn xgboost pyarabic\n",
    "%pip install python-Levenshtein\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables for all and easy accessibility\n",
    "\n",
    "* masterfile: the path of the masterfile that has the correct names\n",
    "* dataset: the path of the desired training data\n",
    "* testfile: the file we want to match data with and test this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "master = \"./Masterfile.xlsx\"\n",
    "training = \"./Dedup_Dataset.xlsx\"\n",
    "test = \"./results.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    \"\"\"Harsh and Kinda extreme preprocessing but for the clarity\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#%\\'()*+,./:;<=>?@[\\\\]^_`{|}~-]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)  # Remove specific words and everything after them\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)  # Remove specific words even if they are part of another word\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)  # Remove repeated letters\n",
    "    return text\n",
    "\n",
    "\n",
    "def optimize_normalize_arabic(text):\n",
    "    \"\"\"Enhanced Arabic text normalization with reduced operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(araby.normalize_ligature(araby.normalize_alef(araby.normalize_teh(text))))\n",
    "    text = re.sub(\n",
    "        r'[^\\w\\s]|(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*|(سعر|جديد|قديم|س ق|س ج)|(.)\\1+',\n",
    "        lambda m: m.group(2) if m.group(2) else '',\n",
    "        text\n",
    "    )\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and its Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. using Binary Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:49:47.105133Z",
     "iopub.status.busy": "2025-02-13T12:49:47.104742Z",
     "iopub.status.idle": "2025-02-13T13:14:52.692164Z",
     "shell.execute_reply": "2025-02-13T13:14:52.690821Z",
     "shell.execute_reply.started": "2025-02-13T12:49:47.105105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def optimize_normalize_arabic(text):\n",
    "    \"\"\"Enhanced Arabic text normalization with reduced operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Combine multiple normalizations in fewer passes\n",
    "    text = araby.normalize_hamza(araby.normalize_ligature(araby.normalize_alef(araby.normalize_teh(text))))\n",
    "    # Combine multiple regex operations\n",
    "    text = re.sub(\n",
    "        r'[^\\w\\s]|(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*|(سعر|جديد|قديم|س ق|س ج)|(.)\\1+',\n",
    "        lambda m: m.group(2) if m.group(2) else '',\n",
    "        text\n",
    "    )\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def compute_optimized_features_batch(pairs, tfidf_vectorizer):\n",
    "    \"\"\"Compute features for multiple pairs at once with corrected vectorization\"\"\"\n",
    "    features = np.zeros((len(pairs), 4))\n",
    "    \n",
    "    # Extract text pairs\n",
    "    texts1, texts2 = zip(*pairs)\n",
    "    \n",
    "    # Compute TF-IDF vectors for all texts at once\n",
    "    tfidf_vectors1 = tfidf_vectorizer.transform(texts1)\n",
    "    tfidf_vectors2 = tfidf_vectorizer.transform(texts2)\n",
    "    \n",
    "    # Compute cosine similarities in one go\n",
    "    cosine_sims = cosine_similarity(tfidf_vectors1, tfidf_vectors2)\n",
    "    \n",
    "    for i, (text1, text2) in enumerate(pairs):\n",
    "        # Fuzzy string matching features\n",
    "        features[i, 0] = fuzz.ratio(text1, text2)\n",
    "        features[i, 1] = fuzz.token_set_ratio(text1, text2)\n",
    "        \n",
    "        # Token overlap\n",
    "        tokens1 = set(text1.split())\n",
    "        tokens2 = set(text2.split())\n",
    "        features[i, 2] = len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 or tokens2 else 0.0\n",
    "        \n",
    "        # TF-IDF cosine similarity\n",
    "        features[i, 3] = cosine_sims[i, i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_optimized_training_data(train_file, master_file, sample_negatives=2):\n",
    "    \"\"\"Optimized training data preparation with corrected batch processing\"\"\"\n",
    "    # Read data efficiently\n",
    "    print(\"Reading data files...\")\n",
    "    train_df = pd.read_excel(train_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file, usecols=['sku', 'product_name_ar'])\n",
    "    \n",
    "    # Parallel text normalization\n",
    "    print(\"Normalizing text...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, train_df['seller_item_name']\n",
    "        ))\n",
    "        master_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, master_df['product_name_ar']\n",
    "        ))\n",
    "    \n",
    "    # Create positive pairs using vectorized operations\n",
    "    print(\"Creating positive pairs...\")\n",
    "    train_master_merged = train_df.merge(\n",
    "        master_df, on='sku', suffixes=('_train', '_master')\n",
    "    )\n",
    "    pos_pairs = list(zip(\n",
    "        train_master_merged['Normalized_Product_train'],\n",
    "        train_master_merged['Normalized_Product_master']\n",
    "    ))\n",
    "    \n",
    "    # Create negative pairs efficiently\n",
    "    print(\"Creating negative pairs...\")\n",
    "    neg_pairs = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        negative_samples = master_df[master_df['sku'] != row['sku']].sample(\n",
    "            n=min(sample_negatives, len(master_df)-1),\n",
    "            random_state=42\n",
    "        )\n",
    "        neg_pairs.extend([\n",
    "            (row['Normalized_Product'], neg_row['Normalized_Product'])\n",
    "            for _, neg_row in negative_samples.iterrows()\n",
    "        ])\n",
    "    \n",
    "    # Combine all pairs\n",
    "    all_pairs = pos_pairs + neg_pairs\n",
    "    \n",
    "    # Prepare TF-IDF vectorizer\n",
    "    print(\"Computing TF-IDF features...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=2,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    # Fit vectorizer on all texts\n",
    "    all_texts = [text for pair in all_pairs for text in pair]\n",
    "    tfidf_vectorizer.fit(all_texts)\n",
    "    \n",
    "    # Process features in batches\n",
    "    print(\"Computing features in batches...\")\n",
    "    batch_size = 1000\n",
    "    X = []\n",
    "    for i in range(0, len(all_pairs), batch_size):\n",
    "        batch_pairs = all_pairs[i:i+batch_size]\n",
    "        batch_features = compute_optimized_features_batch(\n",
    "            batch_pairs,\n",
    "            tfidf_vectorizer\n",
    "        )\n",
    "        X.append(batch_features)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate([\n",
    "        np.ones(len(pos_pairs)),\n",
    "        np.zeros(len(neg_pairs))\n",
    "    ])\n",
    "    \n",
    "    return X, y, tfidf_vectorizer\n",
    "\n",
    "def train_optimized_model(train_file, master_file):\n",
    "    \"\"\"Train model with optimized parameters and early stopping\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, tfidf_vectorizer = prepare_optimized_training_data(train_file, master_file)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to DMatrix for faster training\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model(\"optimized_boosting_model.json\")\n",
    "    print(\"Model saved as optimized_boosting_model.json\")\n",
    "    \n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Model Initialization\n",
    "\n",
    "model, vectorizer = train_optimized_model(training, master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect if text is primarily Arabic or English based on character count\n",
    "    Returns 'arabic' if primarily Arabic, 'english' if primarily English\n",
    "    \"\"\"\n",
    "    # Count English letters (a-z, A-Z)\n",
    "    english_count = len(re.findall(r'[a-zA-Z]', text))\n",
    "    \n",
    "    # If more than 3 English letters, consider it English\n",
    "    if english_count > 3:\n",
    "        return 'english'\n",
    "    return 'arabic'\n",
    "\n",
    "def normalize_english(text):\n",
    "    \"\"\"Normalize English text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common product-related words and everything after them\n",
    "    text = re.sub(r'\\b(?:price|new|old|p n|p o)\\b.*', '', text)\n",
    "    \n",
    "    # Remove repeated letters (e.g., 'goood' -> 'good')\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def fast_normalize_arabic(text):\n",
    "    \"\"\"Extremely fast Arabic text normalization focusing only on critical operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#%\\'()*+,./:;<=>?@[\\\\]^_`{|}~-]', '', text)\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text based on detected language\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    language = detect_language(text)\n",
    "    if language == 'english':\n",
    "        return normalize_english(text)\n",
    "    return fast_normalize_arabic(text)\n",
    "\n",
    "def compute_token_overlap(text1, text2):\n",
    "    \"\"\"Compute token overlap between two texts\"\"\"\n",
    "    tokens1 = set(text1.split())\n",
    "    tokens2 = set(text2.split())\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    return len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "\n",
    "def precompute_tfidf_matrix(texts, vectorizer):\n",
    "    \"\"\"Precompute TF-IDF matrix for all texts\"\"\"\n",
    "    return vectorizer.transform(texts)\n",
    "\n",
    "def compute_batch_features(query_texts, master_texts, query_tfidf, master_tfidf):\n",
    "    \"\"\"Compute features for a batch of text pairs efficiently\"\"\"\n",
    "    # Calculate cosine similarities for the entire batch at once\n",
    "    cosine_sims = cosine_similarity(query_tfidf, master_tfidf)\n",
    "    \n",
    "    n_queries = len(query_texts)\n",
    "    n_masters = len(master_texts)\n",
    "    \n",
    "    # Initialize feature matrices\n",
    "    fuzz_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_set_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_overlaps = np.zeros((n_queries, n_masters))\n",
    "    \n",
    "    # Compute features in parallel\n",
    "    def compute_pair_features(i, j):\n",
    "        return (\n",
    "            fuzz.ratio(query_texts[i], master_texts[j]),\n",
    "            fuzz.token_set_ratio(query_texts[i], master_texts[j]),\n",
    "            compute_token_overlap(query_texts[i], master_texts[j])\n",
    "        )\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i in range(n_queries):\n",
    "            for j in range(n_masters):\n",
    "                futures.append(executor.submit(compute_pair_features, i, j))\n",
    "        \n",
    "        for idx, future in enumerate(futures):\n",
    "            i = idx // n_masters\n",
    "            j = idx % n_masters\n",
    "            ratio, token_set, overlap = future.result()\n",
    "            fuzz_ratios[i, j] = ratio\n",
    "            token_set_ratios[i, j] = token_set\n",
    "            token_overlaps[i, j] = overlap\n",
    "    \n",
    "    features = np.stack([\n",
    "        fuzz_ratios,\n",
    "        token_set_ratios,\n",
    "        token_overlaps,\n",
    "        cosine_sims\n",
    "    ], axis=2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def match_products_bilingual(query_file, master_file, model, vectorizer, threshold=0.5, output_file=\"./MatchedResults_Logistic.xlsx\"):\n",
    "    \"\"\"Bilingual matching function supporting both Arabic and English text\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    \n",
    "    # Load data efficiently\n",
    "    query_df = pd.read_excel(query_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    \n",
    "    print(f\"Loaded {len(query_df)} query products and {len(master_df)} master products\")\n",
    "    \n",
    "    # Detect language for each query product\n",
    "    print(\"Detecting languages and normalizing texts...\")\n",
    "    query_df['detected_language'] = query_df['seller_item_name'].apply(detect_language)\n",
    "    query_df['Normalized_Product'] = query_df['seller_item_name'].apply(normalize_text)\n",
    "    \n",
    "    # Initialize master product columns\n",
    "    master_df['Normalized_Product_AR'] = master_df['product_name_ar'].apply(fast_normalize_arabic)\n",
    "    master_df['Normalized_Product_EN'] = master_df['product_name'].apply(normalize_english)\n",
    "    \n",
    "    # Process Arabic and English queries separately\n",
    "    results = []\n",
    "    \n",
    "    # Handle Arabic queries\n",
    "    arabic_queries = query_df[query_df['detected_language'] == 'arabic']\n",
    "    if len(arabic_queries) > 0:\n",
    "        print(\"Processing Arabic queries...\")\n",
    "        arabic_query_tfidf = precompute_tfidf_matrix(arabic_queries['Normalized_Product'], vectorizer)\n",
    "        arabic_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_AR'], vectorizer)\n",
    "        \n",
    "        arabic_features = compute_batch_features(\n",
    "            arabic_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_AR'].tolist(),\n",
    "            arabic_query_tfidf,\n",
    "            arabic_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process Arabic predictions\n",
    "        arabic_predictions = process_predictions(\n",
    "            arabic_queries,\n",
    "            master_df,\n",
    "            arabic_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(arabic_predictions)\n",
    "    \n",
    "    # Handle English queries\n",
    "    english_queries = query_df[query_df['detected_language'] == 'english']\n",
    "    if len(english_queries) > 0:\n",
    "        print(\"Processing English queries...\")\n",
    "        english_query_tfidf = precompute_tfidf_matrix(english_queries['Normalized_Product'], vectorizer)\n",
    "        english_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_EN'], vectorizer)\n",
    "        \n",
    "        english_features = compute_batch_features(\n",
    "            english_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_EN'].tolist(),\n",
    "            english_query_tfidf,\n",
    "            english_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process English predictions\n",
    "        english_predictions = process_predictions(\n",
    "            english_queries,\n",
    "            master_df,\n",
    "            english_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(english_predictions)\n",
    "    \n",
    "    # Create and save results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"Matching completed in {processing_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(processing_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def process_predictions(query_df, master_df, features, model, threshold):\n",
    "    \"\"\"\n",
    "    Process predictions for a set of queries using XGBoost's predict method\n",
    "    \n",
    "    Parameters:\n",
    "    - query_df: DataFrame containing query products\n",
    "    - master_df: DataFrame containing master products\n",
    "    - features: Computed similarity features\n",
    "    - model: XGBoost model (Booster object)\n",
    "    - threshold: Minimum probability threshold for accepting matches\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing match results\n",
    "    \"\"\"\n",
    "    n_queries, n_masters, n_features = features.shape\n",
    "    features_reshaped = features.reshape(-1, n_features)\n",
    "    \n",
    "    # Convert features to DMatrix for XGBoost\n",
    "    dtest = xgb.DMatrix(features_reshaped)\n",
    "    \n",
    "    # Get raw predictions and convert to probabilities using softmax\n",
    "    predictions = model.predict(dtest)\n",
    "    \n",
    "    # If the model outputs raw scores (not probabilities), convert to probabilities\n",
    "    if len(predictions.shape) == 1:  # If predictions are 1-dimensional\n",
    "        predictions = 1 / (1 + np.exp(-predictions))  # Apply sigmoid for binary classification\n",
    "    else:  # If predictions are 2-dimensional (multiple classes)\n",
    "        predictions = predictions[:, 1]  # Take the probability of class 1\n",
    "    \n",
    "    predictions = predictions.reshape(n_queries, n_masters)\n",
    "    \n",
    "    best_match_indices = np.argmax(predictions, axis=1)\n",
    "    best_match_scores = np.max(predictions, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    for i, (_, query_row) in enumerate(query_df.iterrows()):\n",
    "        if best_match_scores[i] >= threshold:\n",
    "            master_row = master_df.iloc[best_match_indices[i]]\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row['sku'],\n",
    "                \"Query Product\": query_row['seller_item_name'],\n",
    "                \"Matched Master SKU\": master_row['sku'],\n",
    "                \"Matched Master Product\": master_row['product_name_ar'] if query_row['detected_language'] == 'arabic' else master_row['product_name'],\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row['sku'],\n",
    "                \"Query Product\": query_row['seller_item_name'],\n",
    "                \"Matched Master SKU\": None,\n",
    "                \"Matched Master Product\": None,\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_matching_with_timing(query_file, master_file, model, vectorizer):\n",
    "    \"\"\"Run matching with detailed timing information\"\"\"\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = match_products_bilingual(query_file, master_file, model, vectorizer)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(total_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "results = run_matching_with_timing(\n",
    "    test,\n",
    "    master,\n",
    "    model,\n",
    "    vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using rank:pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:35:31.034043Z",
     "iopub.status.busy": "2025-02-13T12:35:31.033629Z",
     "iopub.status.idle": "2025-02-13T12:36:06.541387Z",
     "shell.execute_reply": "2025-02-13T12:36:06.540442Z",
     "shell.execute_reply.started": "2025-02-13T12:35:31.034011Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 501\n",
      "Master products: 1000\n",
      "Found 500 matching SKUs\n",
      "Normalizing text...\n",
      "Initializing TF-IDF vectorizer...\n",
      "Creating ranking groups...\n",
      "Processing query 0/500\n",
      "Processing query 43400/500\n",
      "Processing query 44600/500\n",
      "Processing query 46600/500\n",
      "Created 500 ranking groups\n",
      "Total pairs: 3000\n",
      "Positive pairs: 500.0\n",
      "Negative pairs: 2500.0\n",
      "Splitting data...\n",
      "Training model...\n",
      "[0]\ttrain-ndcg@5:0.99137\ttrain-map@5:0.98833\tval-ndcg@5:0.99631\tval-map@5:0.99500\n",
      "[10]\ttrain-ndcg@5:0.99815\ttrain-map@5:0.99750\tval-ndcg@5:0.98818\tval-map@5:0.98450\n",
      "Model saved as ranking_product_matcher.json\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_optimized_features_batch(pairs, tfidf_vectorizer):\n",
    "    \"\"\"Compute features for multiple pairs at once with vectorization\"\"\"\n",
    "    features = np.zeros((len(pairs), 4))\n",
    "    \n",
    "    texts1, texts2 = zip(*pairs)\n",
    "    \n",
    "    tfidf_vectors1 = tfidf_vectorizer.transform(texts1)\n",
    "    tfidf_vectors2 = tfidf_vectorizer.transform(texts2)\n",
    "    \n",
    "    cosine_sims = cosine_similarity(tfidf_vectors1, tfidf_vectors2)\n",
    "    \n",
    "    for i, (text1, text2) in enumerate(pairs):\n",
    "        features[i, 0] = fuzz.ratio(text1, text2)\n",
    "        features[i, 1] = fuzz.token_set_ratio(text1, text2)\n",
    "        \n",
    "        tokens1 = set(text1.split())\n",
    "        tokens2 = set(text2.split())\n",
    "        features[i, 2] = len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 or tokens2 else 0.0\n",
    "        features[i, 3] = cosine_sims[i, i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_ranking_training_data(train_file, master_file, neg_samples_per_query=5):\n",
    "    \"\"\"\n",
    "    Prepare training data for learning to rank approach with robust data validation\n",
    "    \"\"\"\n",
    "    print(\"Reading data files...\")\n",
    "    train_df = pd.read_excel(train_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file, usecols=['sku', 'product_name_ar'])\n",
    "    \n",
    "    # Remove any duplicates and null values\n",
    "    train_df = train_df.dropna(subset=['sku', 'seller_item_name']).drop_duplicates(subset=['sku'])\n",
    "    master_df = master_df.dropna(subset=['sku', 'product_name_ar']).drop_duplicates(subset=['sku'])\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Master products: {len(master_df)}\")\n",
    "    \n",
    "    # Find matching SKUs between training and master data\n",
    "    matching_skus = set(train_df['sku']).intersection(set(master_df['sku']))\n",
    "    print(f\"Found {len(matching_skus)} matching SKUs\")\n",
    "    \n",
    "    if len(matching_skus) == 0:\n",
    "        raise ValueError(\"No matching SKUs found between training and master data\")\n",
    "    \n",
    "    # Filter to only matching SKUs\n",
    "    train_df = train_df[train_df['sku'].isin(matching_skus)]\n",
    "    \n",
    "    print(\"Normalizing text...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, train_df['seller_item_name']\n",
    "        ))\n",
    "        master_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, master_df['product_name_ar']\n",
    "        ))\n",
    "    \n",
    "    print(\"Initializing TF-IDF vectorizer...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=2,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    all_texts = pd.concat([train_df['Normalized_Product'], master_df['Normalized_Product']]).unique()\n",
    "    tfidf_vectorizer.fit(all_texts)\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "        pickle.dump(tfidf_vectorizer, file)\n",
    "    \n",
    "    print(\"Creating ranking groups...\")\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    groups_list = []\n",
    "    \n",
    "    # Create a master product lookup dictionary for efficiency\n",
    "    master_lookup = master_df.set_index('sku')['Normalized_Product'].to_dict()\n",
    "    \n",
    "    for idx, query_row in train_df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing query {idx}/{len(train_df)}\")\n",
    "            \n",
    "        query_sku = query_row['sku']\n",
    "        query_text = query_row['Normalized_Product']\n",
    "        \n",
    "        # Get positive match from lookup dictionary\n",
    "        positive_match = master_lookup[query_sku]\n",
    "        \n",
    "        # Sample negative matches from products with different SKUs\n",
    "        negative_skus = np.random.choice(\n",
    "            [sku for sku in master_lookup.keys() if sku != query_sku],\n",
    "            size=min(neg_samples_per_query, len(master_lookup)-1),\n",
    "            replace=False\n",
    "        )\n",
    "        negative_matches = [master_lookup[sku] for sku in negative_skus]\n",
    "        \n",
    "        # Create pairs for this query\n",
    "        all_pairs = [(query_text, positive_match)] + [(query_text, neg) for neg in negative_matches]\n",
    "        \n",
    "        # Compute features for all pairs in this group\n",
    "        group_features = compute_optimized_features_batch(all_pairs, tfidf_vectorizer)\n",
    "        \n",
    "        # Create labels (1 for positive match, 0 for negative matches)\n",
    "        group_labels = np.zeros(len(all_pairs))\n",
    "        group_labels[0] = 1\n",
    "        \n",
    "        features_list.append(group_features)\n",
    "        labels_list.append(group_labels)\n",
    "        groups_list.append(len(all_pairs))\n",
    "    \n",
    "    if not features_list:\n",
    "        raise ValueError(\"No valid training pairs could be created\")\n",
    "    \n",
    "    X = np.vstack(features_list)\n",
    "    y = np.concatenate(labels_list)\n",
    "    groups = np.array(groups_list)\n",
    "    \n",
    "    print(f\"Created {len(groups)} ranking groups\")\n",
    "    print(f\"Total pairs: {len(X)}\")\n",
    "    print(f\"Positive pairs: {sum(y)}\")\n",
    "    print(f\"Negative pairs: {len(y) - sum(y)}\")\n",
    "    \n",
    "    return X, y, groups, tfidf_vectorizer\n",
    "\n",
    "def train_ranking_model(train_file, master_file):\n",
    "    \"\"\"Train a ranking-based model using XGBoost's ranking objective\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, groups, tfidf_vectorizer = prepare_ranking_training_data(train_file, master_file)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    unique_groups = np.cumsum(groups)\n",
    "    n_groups = len(groups)\n",
    "    train_idx = np.random.choice(n_groups, int(0.8 * n_groups), replace=False)\n",
    "    val_idx = np.array(list(set(range(n_groups)) - set(train_idx)))\n",
    "    \n",
    "    train_mask = np.zeros(len(X), dtype=bool)\n",
    "    val_mask = np.zeros(len(X), dtype=bool)\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(n_groups):\n",
    "        end = start + groups[i]\n",
    "        if i in train_idx:\n",
    "            train_mask[start:end] = True\n",
    "        else:\n",
    "            val_mask[start:end] = True\n",
    "        start = end\n",
    "    \n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    groups_train = groups[train_idx]\n",
    "    groups_val = groups[val_idx]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtrain.set_group(groups_train)\n",
    "    dval.set_group(groups_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'rank:pairwise',\n",
    "        'eval_metric': ['ndcg@5', 'map@5'],\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    model.save_model(\"ranking_product_matcher.json\")\n",
    "    print(\"Model saved as ranking_product_matcher.json\")\n",
    "    \n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "model, vectorizer = train_ranking_model(training, master)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:40:13.737189Z",
     "iopub.status.busy": "2025-02-13T12:40:13.736816Z",
     "iopub.status.idle": "2025-02-13T12:40:39.171068Z",
     "shell.execute_reply": "2025-02-13T12:40:39.170006Z",
     "shell.execute_reply.started": "2025-02-13T12:40:13.737145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bilingual matching process...\n",
      "Starting bilingual matching process...\n",
      "Loaded 72 query products and 1000 master products\n",
      "Detecting languages and normalizing texts...\n",
      "Processing Arabic queries...\n",
      "Processing English queries...\n",
      "Matching completed in 3.90 seconds\n",
      "Average time per product: 54.23 ms\n",
      "\n",
      "Performance Summary:\n",
      "Total processing time: 3.90 seconds\n",
      "Average time per product: 54.23 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect if text is primarily Arabic or English based on character count\n",
    "    Returns 'arabic' if primarily Arabic, 'english' if primarily English\n",
    "    \"\"\"\n",
    "    # Count English letters (a-z, A-Z)\n",
    "    english_count = len(re.findall(r'[a-zA-Z]', text))\n",
    "    \n",
    "    # If more than 3 English letters, consider it English\n",
    "    if english_count > 3:\n",
    "        return 'english'\n",
    "    return 'arabic'\n",
    "\n",
    "def normalize_english(text):\n",
    "    \"\"\"Normalize English text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common product-related words and everything after them\n",
    "    text = re.sub(r'\\b(?:price|new|old|p n|p o)\\b.*', '', text)\n",
    "    \n",
    "    # Remove repeated letters (e.g., 'goood' -> 'good')\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def fast_normalize_arabic(text):\n",
    "    \"\"\"Extremely fast Arabic text normalization focusing only on critical operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#%\\'()*+,./:;<=>?@[\\\\]^_`{|}~-]', '', text)\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text based on detected language\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    language = detect_language(text)\n",
    "    if language == 'english':\n",
    "        return normalize_english(text)\n",
    "    return fast_normalize_arabic(text)\n",
    "\n",
    "def compute_token_overlap(text1, text2):\n",
    "    \"\"\"Compute token overlap between two texts\"\"\"\n",
    "    tokens1 = set(text1.split())\n",
    "    tokens2 = set(text2.split())\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    return len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "\n",
    "def precompute_tfidf_matrix(texts, vectorizer):\n",
    "    \"\"\"Precompute TF-IDF matrix for all texts\"\"\"\n",
    "    return vectorizer.transform(texts)\n",
    "\n",
    "def compute_batch_features(query_texts, master_texts, query_tfidf, master_tfidf):\n",
    "    \"\"\"Compute features for a batch of text pairs efficiently\"\"\"\n",
    "    # Calculate cosine similarities for the entire batch at once\n",
    "    cosine_sims = cosine_similarity(query_tfidf, master_tfidf)\n",
    "    \n",
    "    n_queries = len(query_texts)\n",
    "    n_masters = len(master_texts)\n",
    "    \n",
    "    # Initialize feature matrices\n",
    "    fuzz_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_set_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_overlaps = np.zeros((n_queries, n_masters))\n",
    "    \n",
    "    # Compute features in parallel\n",
    "    def compute_pair_features(i, j):\n",
    "        return (\n",
    "            fuzz.ratio(query_texts[i], master_texts[j]),\n",
    "            fuzz.token_set_ratio(query_texts[i], master_texts[j]),\n",
    "            compute_token_overlap(query_texts[i], master_texts[j])\n",
    "        )\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i in range(n_queries):\n",
    "            for j in range(n_masters):\n",
    "                futures.append(executor.submit(compute_pair_features, i, j))\n",
    "        \n",
    "        for idx, future in enumerate(futures):\n",
    "            i = idx // n_masters\n",
    "            j = idx % n_masters\n",
    "            ratio, token_set, overlap = future.result()\n",
    "            fuzz_ratios[i, j] = ratio\n",
    "            token_set_ratios[i, j] = token_set\n",
    "            token_overlaps[i, j] = overlap\n",
    "    \n",
    "    features = np.stack([\n",
    "        fuzz_ratios,\n",
    "        token_set_ratios,\n",
    "        token_overlaps,\n",
    "        cosine_sims\n",
    "    ], axis=2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def match_products_bilingual(query_file, master_file, model, vectorizer, threshold=0.5, output_file=\"MatchedResults_Bilingual.xlsx\"):\n",
    "    \"\"\"Bilingual matching function supporting both Arabic and English text\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    \n",
    "    # Load data efficiently\n",
    "    query_df = pd.read_excel(query_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    \n",
    "    print(f\"Loaded {len(query_df)} query products and {len(master_df)} master products\")\n",
    "    \n",
    "    # Detect language for each query product\n",
    "    print(\"Detecting languages and normalizing texts...\")\n",
    "    query_df['detected_language'] = query_df['seller_item_name'].apply(detect_language)\n",
    "    query_df['Normalized_Product'] = query_df['seller_item_name'].apply(normalize_text)\n",
    "    \n",
    "    # Initialize master product columns\n",
    "    master_df['Normalized_Product_AR'] = master_df['product_name_ar'].apply(fast_normalize_arabic)\n",
    "    master_df['Normalized_Product_EN'] = master_df['product_name'].apply(normalize_english)\n",
    "    \n",
    "    # Process Arabic and English queries separately\n",
    "    results = []\n",
    "    \n",
    "    # Handle Arabic queries\n",
    "    arabic_queries = query_df[query_df['detected_language'] == 'arabic']\n",
    "    if len(arabic_queries) > 0:\n",
    "        print(\"Processing Arabic queries...\")\n",
    "        arabic_query_tfidf = precompute_tfidf_matrix(arabic_queries['Normalized_Product'], vectorizer)\n",
    "        arabic_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_AR'], vectorizer)\n",
    "        \n",
    "        arabic_features = compute_batch_features(\n",
    "            arabic_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_AR'].tolist(),\n",
    "            arabic_query_tfidf,\n",
    "            arabic_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process Arabic predictions\n",
    "        arabic_predictions = process_predictions(\n",
    "            arabic_queries,\n",
    "            master_df,\n",
    "            arabic_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(arabic_predictions)\n",
    "    \n",
    "    # Handle English queries\n",
    "    english_queries = query_df[query_df['detected_language'] == 'english']\n",
    "    if len(english_queries) > 0:\n",
    "        print(\"Processing English queries...\")\n",
    "        english_query_tfidf = precompute_tfidf_matrix(english_queries['Normalized_Product'], vectorizer)\n",
    "        english_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_EN'], vectorizer)\n",
    "        \n",
    "        english_features = compute_batch_features(\n",
    "            english_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_EN'].tolist(),\n",
    "            english_query_tfidf,\n",
    "            english_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process English predictions\n",
    "        english_predictions = process_predictions(\n",
    "            english_queries,\n",
    "            master_df,\n",
    "            english_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(english_predictions)\n",
    "    \n",
    "    # Create and save results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"Matching completed in {processing_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(processing_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def process_predictions(query_df, master_df, features, model, threshold):\n",
    "    \"\"\"\n",
    "    Process predictions for a set of queries using XGBoost's predict method\n",
    "    \n",
    "    Parameters:\n",
    "    - query_df: DataFrame containing query products\n",
    "    - master_df: DataFrame containing master products\n",
    "    - features: Computed similarity features\n",
    "    - model: XGBoost model (Booster object)\n",
    "    - threshold: Minimum probability threshold for accepting matches\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing match results\n",
    "    \"\"\"\n",
    "    n_queries, n_masters, n_features = features.shape\n",
    "    features_reshaped = features.reshape(-1, n_features)\n",
    "    \n",
    "    # Convert features to DMatrix for XGBoost\n",
    "    dtest = xgb.DMatrix(features_reshaped)\n",
    "    \n",
    "    # Get raw predictions and convert to probabilities using softmax\n",
    "    predictions = model.predict(dtest)\n",
    "    \n",
    "    # If the model outputs raw scores (not probabilities), convert to probabilities\n",
    "    if len(predictions.shape) == 1:  # If predictions are 1-dimensional\n",
    "        predictions = 1 / (1 + np.exp(-predictions))  # Apply sigmoid for binary classification\n",
    "    else:  # If predictions are 2-dimensional (multiple classes)\n",
    "        predictions = predictions[:, 1]  # Take the probability of class 1\n",
    "    \n",
    "    predictions = predictions.reshape(n_queries, n_masters)\n",
    "    \n",
    "    best_match_indices = np.argmax(predictions, axis=1)\n",
    "    best_match_scores = np.max(predictions, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    for i, (_, query_row) in enumerate(query_df.iterrows()):\n",
    "        if best_match_scores[i] >= threshold:\n",
    "            master_row = master_df.iloc[best_match_indices[i]]\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row['sku'],\n",
    "                \"Query Product\": query_row['seller_item_name'],\n",
    "                \"Matched Master SKU\": master_row['sku'],\n",
    "                \"Matched Master Product\": master_row['product_name_ar'] if query_row['detected_language'] == 'arabic' else master_row['product_name'],\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row['sku'],\n",
    "                \"Query Product\": query_row['seller_item_name'],\n",
    "                \"Matched Master SKU\": None,\n",
    "                \"Matched Master Product\": None,\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_matching_with_timing(query_file, master_file, model, vectorizer):\n",
    "    \"\"\"Run matching with detailed timing information\"\"\"\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = match_products_bilingual(query_file, master_file, model, vectorizer)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(total_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = run_matching_with_timing(\n",
    "    test,\n",
    "    master,\n",
    "    model,\n",
    "    vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6608149,
     "sourceId": 10739351,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6653528,
     "sourceId": 10731569,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
