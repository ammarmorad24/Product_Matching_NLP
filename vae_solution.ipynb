{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install torch sentence-transformers pyarabic nltk\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Rest of your code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class ArabicTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def remove_tashkeel(self, text):\n",
    "        \"\"\"Remove Arabic diacritics from text.\"\"\"\n",
    "        return araby.strip_tashkeel(text)\n",
    "    \n",
    "    def remove_tatweel(self, text):\n",
    "        \"\"\"Remove Arabic text elongation.\"\"\"\n",
    "        return araby.strip_tatweel(text)\n",
    "    \n",
    "    \n",
    "    def normalize_hamza(self, text):\n",
    "        \"\"\"Normalize different forms of Hamza.\"\"\"\n",
    "        text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "        text = re.sub(\"ى\", \"ي\", text)\n",
    "        text = re.sub(\"ؤ\", \"ء\", text)\n",
    "        text = re.sub(\"ئ\", \"ء\", text)\n",
    "        return text\n",
    "    \n",
    "    def remove_special_chars(self, text):\n",
    "        \"\"\"Remove special characters and non-Arabic letters.\"\"\"\n",
    "        # Remove non-linguistic characters like * / # etc.\n",
    "        text = re.sub(r'[^\\w\\sاأإآء-ي]', ' ', text)\n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Apply full preprocessing pipeline.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Normalize Arabic text\n",
    "        text = text.lower()\n",
    "        text = self.remove_tashkeel(text)\n",
    "        text = self.remove_tatweel(text)\n",
    "        text = self.normalize_hamza(text)\n",
    "        \n",
    "        # Remove special characters and non-linguistic symbols\n",
    "        text = self.remove_special_chars(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "class EnhancedVAELoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=0.5, gamma=0.3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Reconstruction weight\n",
    "        self.beta = beta   # KL divergence weight\n",
    "        self.gamma = gamma # Similarity preservation weight\n",
    "        \n",
    "    def forward(self, recon_x, x, mu, log_var, z):\n",
    "        # Reconstruction loss with cosine similarity\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        cos_sim_loss = 1 - F.cosine_similarity(recon_x, x, dim=1).mean()\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Similarity preservation loss in latent space\n",
    "        z_norm = F.normalize(z, p=2, dim=1)\n",
    "        similarity_matrix = torch.mm(z_norm, z_norm.t())\n",
    "        original_norm = F.normalize(x, p=2, dim=1)\n",
    "        original_similarity = torch.mm(original_norm, original_norm.t())\n",
    "        similarity_preservation_loss = F.mse_loss(similarity_matrix, original_similarity)\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = (\n",
    "            self.alpha * (recon_loss + cos_sim_loss) + \n",
    "            self.beta * kl_loss + \n",
    "            self.gamma * similarity_preservation_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, {\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'cos_sim_loss': cos_sim_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'similarity_loss': similarity_preservation_loss.item()\n",
    "        }\n",
    "\n",
    "class ProductVAE(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim=256, latent_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Enhanced encoder with residual connections\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # VAE components\n",
    "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        \n",
    "        # Enhanced decoder with residual connections\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim // 2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var, z\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class EnhancedProductDataset(Dataset):\n",
    "    def __init__(self, df, bert_model, preprocessor):\n",
    "        self.model = bert_model\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "        # Preprocess and create embeddings\n",
    "        texts = df['seller_item_name'].apply(\n",
    "            lambda x: self.preprocessor.preprocess(x)\n",
    "        ).tolist()\n",
    "        \n",
    "        self.embeddings = torch.tensor(\n",
    "            self.model.encode(texts, convert_to_numpy=True),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx]\n",
    "\n",
    "def train_enhanced_vae(model, train_loader, optimizer, device, epochs=10):\n",
    "    criterion = EnhancedVAELoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_metrics = {'total_loss': 0, 'recon_loss': 0, \n",
    "                        'cos_sim_loss': 0, 'kl_loss': 0, \n",
    "                        'similarity_loss': 0}\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon_batch, mu, log_var, z = model(data)\n",
    "            loss, metrics = criterion(recon_batch, data, mu, log_var, z)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_metrics['total_loss'] += loss.item()\n",
    "            for key, value in metrics.items():\n",
    "                total_metrics[key] += value\n",
    "        \n",
    "        # Print epoch metrics\n",
    "        print(f'\\nEpoch: {epoch+1}')\n",
    "        for key, value in total_metrics.items():\n",
    "            avg_value = value / len(train_loader.dataset)\n",
    "            print(f'Average {key}: {avg_value:.4f}')\n",
    "\n",
    "def match_products(query_text, master_df, vae_model, bert_model, preprocessor, device, top_k=5):\n",
    "    vae_model.eval()\n",
    "    \n",
    "    # Preprocess query\n",
    "    processed_query = preprocessor.preprocess(query_text)\n",
    "    query_embedding = torch.tensor(\n",
    "        bert_model.encode([processed_query]),\n",
    "        dtype=torch.float32\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get query latent representation\n",
    "    with torch.no_grad():\n",
    "        query_mu, _ = vae_model.encode(query_embedding)\n",
    "    \n",
    "    # Process master products\n",
    "    master_texts = master_df['product_name_ar'].apply(\n",
    "        lambda x: preprocessor.preprocess(x)\n",
    "    ).tolist()\n",
    "    \n",
    "    master_embeddings = torch.tensor(\n",
    "        bert_model.encode(master_texts),\n",
    "        dtype=torch.float32\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get master products latent representations\n",
    "    with torch.no_grad():\n",
    "        master_mu, _ = vae_model.encode(master_embeddings)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = F.cosine_similarity(\n",
    "        query_mu.unsqueeze(0).expand(len(master_mu), -1),\n",
    "        master_mu,\n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # Get top-k matches\n",
    "    top_k_scores, top_k_indices = torch.topk(similarities, min(top_k, len(master_df)))\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            'product': master_df.iloc[idx]['product_name_ar'],\n",
    "            'score': score.item(),\n",
    "            'processed_text': master_texts[idx]\n",
    "        }\n",
    "        for score, idx in zip(top_k_scores, top_k_indices)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define file paths (update these paths as needed)\n",
    "    training_file = \"/kaggle/input/augmented-dataset/PreDataset.xlsx\"\n",
    "    master_file = \"/kaggle/input/product-matching-dataset/Masterfile.xlsx\"\n",
    "    test_file = \"/kaggle/input/test-set-data/augmented_test_set.xlsx\"\n",
    "    output_file = \"/kaggle/working/camel_results.xlsx\"\n",
    "    \n",
    "    # Load datasets (replace with actual loading code)\n",
    "    import pandas as pd\n",
    "    training_df = pd.read_excel(training_file)\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    \n",
    "    # Initialize components\n",
    "    preprocessor = ArabicTextPreprocessor()\n",
    "    bert_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    vae_model = ProductVAE(\n",
    "        embedding_dim=bert_model.get_sentence_embedding_dimension(),\n",
    "        hidden_dim=256,\n",
    "        latent_dim=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset with preprocessing\n",
    "    dataset = EnhancedProductDataset(training_df, bert_model, preprocessor)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train with enhanced loss\n",
    "    optimizer = torch.optim.AdamW(vae_model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    train_enhanced_vae(vae_model, train_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def match_dataset_vae(master_file, dataset_file, output_file, vae_model, encoder, preprocessor, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match products using the trained VAE model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vae_model = vae_model.to(device)\n",
    "    vae_model.eval()\n",
    "    \n",
    "    # Load data\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    dataset_df = pd.read_excel(dataset_file)\n",
    "    \n",
    "    # Normalize text using the preprocessor\n",
    "    master_df['Normalized Name'] = master_df['product_name_ar'].apply(preprocessor.preprocess)\n",
    "    dataset_df['Normalized Name'] = dataset_df['seller_item_name'].apply(preprocessor.preprocess)\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for master products...\")\n",
    "    master_embeddings = encoder.encode(master_df['Normalized Name'].tolist())\n",
    "    print(\"Creating embeddings for dataset products...\")\n",
    "    dataset_embeddings = encoder.encode(dataset_df['Normalized Name'].tolist())\n",
    "    \n",
    "    # Convert embeddings to tensors\n",
    "    master_embeddings = torch.tensor(master_embeddings, dtype=torch.float32).to(device)\n",
    "    dataset_embeddings = torch.tensor(dataset_embeddings, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Get latent representations using the VAE\n",
    "    print(\"Encoding embeddings into latent space...\")\n",
    "    with torch.no_grad():\n",
    "        master_mu, _ = vae_model.encode(master_embeddings)  # ✅ Fix\n",
    "        dataset_mu, _ = vae_model.encode(dataset_embeddings)  # ✅ Fix\n",
    "    \n",
    "    results = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(\"Finding matches...\")\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset_df), batch_size)):\n",
    "            dataset_batch = dataset_mu[i:i + batch_size]\n",
    "            \n",
    "            # Compute cosine similarity between dataset batch and all master embeddings\n",
    "            similarities = F.cosine_similarity(\n",
    "                dataset_batch.unsqueeze(1),  # Shape: (batch_size, 1, latent_dim)\n",
    "                master_mu.unsqueeze(0),     # Shape: (1, num_master, latent_dim)\n",
    "                dim=-1\n",
    "            )  # Shape: (batch_size, num_master)\n",
    "            \n",
    "            # Get the best match for each item in the batch\n",
    "            max_scores, max_indices = torch.max(similarities, dim=1)\n",
    "            \n",
    "            for k, (max_score, max_idx) in enumerate(zip(max_scores, max_indices)):\n",
    "                if i + k >= len(dataset_df):\n",
    "                    break\n",
    "                    \n",
    "                max_idx = max_idx.item()  # ✅ Convert tensor to integer\n",
    "\n",
    "                results.append({\n",
    "                    'Seller Item': dataset_df.iloc[i + k]['product_name_ar'],\n",
    "                    'Matched Item': master_df.iloc[max_idx]['product_name_ar'],  # ✅ Fix\n",
    "                    'sku': master_df.iloc[max_idx]['sku'],  # ✅ Fix\n",
    "                    'Match Score': max_score.item(),\n",
    "                    'Confidence': 'High' if max_score.item() > threshold else 'Low'\n",
    "                })\n",
    "    \n",
    "    # Save results\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    print(f\"Matching completed. Results saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Define file paths\n",
    "        master_file = \"/kaggle/input/product-matching-dataset/Masterfile.xlsx\"\n",
    "        dataset_file = \"/kaggle/input/manual-testing/results.xlsx\"\n",
    "        output_file = \"/kaggle/working/vae_results.xlsx\"\n",
    "        \n",
    "        # Initialize components\n",
    "        preprocessor = ArabicTextPreprocessor()\n",
    "        encoder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        \n",
    "        # Perform matching\n",
    "        print(\"Performing product matching...\")\n",
    "        match_dataset_vae(master_file, dataset_file, output_file, vae_model, encoder, preprocessor)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
