{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables for Easy Accessibility\n",
    "\n",
    "* **master**: Path to the master file containing the correct names.\n",
    "* **training**: Path to the desired training data.\n",
    "* **test**: File to match data with and test the model.\n",
    "\n",
    "### Column Requirements\n",
    "\n",
    "- **Master File**: Columns should be in the format `(sku, product_name_ar, product_name)`.\n",
    "- **Training File**: Columns should be in the format `(sku, ..., seller_item_name)`.  // Look at Dedup_Dataset.xlsx structure\n",
    "- **Test File**: Columns should be in the format `(item code , product name , price)`.\n",
    "  added a dynamic naming for the test file\n",
    "\n",
    "The model uses the `sku` in the master file to find matches, so only the `sku` is needed in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"./Masterfile.xlsx\"\n",
    "training = \"./Dedup_Dataset.xlsx\"\n",
    "test = \"./test.xlsx\"\n",
    "\n",
    "# dynamic test column names\n",
    "ITEM_NAME = \"product name\"\n",
    "ITEM_CODE = \"item code\"\n",
    "ITEM_PRICE = \"price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Preprocessing\n",
    "\n",
    "The two functions provided serve the same purpose. The first function is written in a more readable format to illustrate the approach taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    \"\"\"Harsh and Kinda extreme preprocessing but for the clarity\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#\\'()*+,.:;<=>?@[\\\\]^_`{|}~]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)  # Remove specific words and everything after them\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)  # Remove specific words even if they are part of another word\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)  # Remove repeated letters\n",
    "    return text\n",
    "\n",
    "\n",
    "def optimize_normalize_arabic(text):\n",
    "    \"\"\"Enhanced Arabic text normalization that preserves % and / characters\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(araby.normalize_ligature(araby.normalize_alef(araby.normalize_teh(text))))\n",
    "    \n",
    "    # Replace unwanted characters except % and /\n",
    "    text = re.sub(r'[^\\w\\s%/]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove specific Arabic terms and everything after them\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)\n",
    "    \n",
    "    # Remove specific terms even if they're part of another word\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)\n",
    "    \n",
    "    # Remove repeated letters\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and its Features\n",
    "## 1. using Binary Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimized_features_batch(pairs, tfidf_vectorizer):\n",
    "    \"\"\"Compute features for multiple pairs at once with corrected vectorization\"\"\"\n",
    "    features = np.zeros((len(pairs), 4))\n",
    "    \n",
    "    # Extract text pairs\n",
    "    texts1, texts2 = zip(*pairs)\n",
    "    \n",
    "    # Compute TF-IDF vectors for all texts at once\n",
    "    tfidf_vectors1 = tfidf_vectorizer.transform(texts1)\n",
    "    tfidf_vectors2 = tfidf_vectorizer.transform(texts2)\n",
    "    \n",
    "    # Compute cosine similarities in one go\n",
    "    cosine_sims = cosine_similarity(tfidf_vectors1, tfidf_vectors2)\n",
    "    \n",
    "    for i, (text1, text2) in enumerate(pairs):\n",
    "        # Fuzzy string matching features\n",
    "        features[i, 0] = fuzz.ratio(text1, text2)\n",
    "        features[i, 1] = fuzz.token_set_ratio(text1, text2)\n",
    "        \n",
    "        # Token overlap\n",
    "        tokens1 = set(text1.split())\n",
    "        tokens2 = set(text2.split())\n",
    "        features[i, 2] = len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 or tokens2 else 0.0\n",
    "        \n",
    "        # TF-IDF cosine similarity\n",
    "        features[i, 3] = cosine_sims[i, i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_optimized_training_data(train_file, master_file, sample_negatives=2):\n",
    "    \"\"\"Optimized training data preparation with corrected batch processing\"\"\"\n",
    "    # Read data efficiently\n",
    "    print(\"Reading data files...\")\n",
    "    train_df = pd.read_excel(train_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file, usecols=['sku', 'product_name_ar'])\n",
    "    \n",
    "    # Parallel text normalization\n",
    "    print(\"Normalizing text...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, train_df['seller_item_name']\n",
    "        ))\n",
    "        master_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, master_df['product_name_ar']\n",
    "        ))\n",
    "    \n",
    "    # Create positive pairs using vectorized operations\n",
    "    print(\"Creating positive pairs...\")\n",
    "    train_master_merged = train_df.merge(\n",
    "        master_df, on='sku', suffixes=('_train', '_master')\n",
    "    )\n",
    "    pos_pairs = list(zip(\n",
    "        train_master_merged['Normalized_Product_train'],\n",
    "        train_master_merged['Normalized_Product_master']\n",
    "    ))\n",
    "    \n",
    "    # Create negative pairs efficiently\n",
    "    print(\"Creating negative pairs...\")\n",
    "    neg_pairs = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        negative_samples = master_df[master_df['sku'] != row['sku']].sample(\n",
    "            n=min(sample_negatives, len(master_df)-1),\n",
    "            random_state=42\n",
    "        )\n",
    "        neg_pairs.extend([\n",
    "            (row['Normalized_Product'], neg_row['Normalized_Product'])\n",
    "            for _, neg_row in negative_samples.iterrows()\n",
    "        ])\n",
    "    \n",
    "    # Combine all pairs\n",
    "    all_pairs = pos_pairs + neg_pairs\n",
    "    \n",
    "    # Prepare TF-IDF vectorizer\n",
    "    print(\"Computing TF-IDF features...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=2,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    # Fit vectorizer on all texts\n",
    "    all_texts = [text for pair in all_pairs for text in pair]\n",
    "    tfidf_vectorizer.fit(all_texts)\n",
    "    \n",
    "    # Process features in batches\n",
    "    print(\"Computing features in batches...\")\n",
    "    batch_size = 1000\n",
    "    X = []\n",
    "    for i in range(0, len(all_pairs), batch_size):\n",
    "        batch_pairs = all_pairs[i:i+batch_size]\n",
    "        batch_features = compute_optimized_features_batch(\n",
    "            batch_pairs,\n",
    "            tfidf_vectorizer\n",
    "        )\n",
    "        X.append(batch_features)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate([\n",
    "        np.ones(len(pos_pairs)),\n",
    "        np.zeros(len(neg_pairs))\n",
    "    ])\n",
    "    \n",
    "    return X, y, tfidf_vectorizer\n",
    "\n",
    "def train_optimized_model(train_file, master_file):\n",
    "    \"\"\"Train model with optimized parameters and early stopping\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, tfidf_vectorizer = prepare_optimized_training_data(train_file, master_file)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to DMatrix for faster training\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model(\"logistic_boosting_model.json\")\n",
    "    print(\"Model saved as logistic_boosting_model.json\")\n",
    "    \n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Reading data files...\n",
      "Normalizing text...\n",
      "Creating positive pairs...\n",
      "Creating negative pairs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Model Initialization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model, vectorizer = \u001b[43mtrain_optimized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mtrain_optimized_model\u001b[39m\u001b[34m(train_file, master_file)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Train model with optimized parameters and early stopping\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreparing training data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m X, y, tfidf_vectorizer = \u001b[43mprepare_optimized_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSplitting data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m X_train, X_val, y_train, y_val = train_test_split(\n\u001b[32m    113\u001b[39m     X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m\n\u001b[32m    114\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mprepare_optimized_training_data\u001b[39m\u001b[34m(train_file, master_file, sample_negatives)\u001b[39m\n\u001b[32m     59\u001b[39m neg_pairs = []\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m train_df.iterrows():\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     negative_samples = \u001b[43mmaster_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmaster_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msku\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msku\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmaster_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     neg_pairs.extend([\n\u001b[32m     66\u001b[39m         (row[\u001b[33m'\u001b[39m\u001b[33mNormalized_Product\u001b[39m\u001b[33m'\u001b[39m], neg_row[\u001b[33m'\u001b[39m\u001b[33mNormalized_Product\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _, neg_row \u001b[38;5;129;01min\u001b[39;00m negative_samples.iterrows()\n\u001b[32m     68\u001b[39m     ])\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Combine all pairs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:6119\u001b[39m, in \u001b[36mNDFrame.sample\u001b[39m\u001b[34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[39m\n\u001b[32m   6116\u001b[39m     weights = sample.preprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[32m   6118\u001b[39m sampled_indices = sample.sample(obj_len, size, replace, weights, rs)\n\u001b[32m-> \u001b[39m\u001b[32m6119\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[32m   6122\u001b[39m     result.index = default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4133\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4128\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4129\u001b[39m     indices = np.arange(\n\u001b[32m   4130\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4131\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4133\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4135\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4139\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4140\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:891\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    888\u001b[39m \u001b[38;5;66;03m# Caller is responsible for ensuring indexer annotation is accurate\u001b[39;00m\n\u001b[32m    890\u001b[39m n = \u001b[38;5;28mself\u001b[39m.shape[axis]\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m indexer = \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reindex_indexer(\n\u001b[32m    895\u001b[39m     new_axis=new_labels,\n\u001b[32m    896\u001b[39m     indexer=indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    899\u001b[39m     copy=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    900\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexers/utils.py:275\u001b[39m, in \u001b[36mmaybe_convert_indices\u001b[39m\u001b[34m(indices, n, verify)\u001b[39m\n\u001b[32m    272\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.empty(\u001b[32m0\u001b[39m, dtype=np.intp)\n\u001b[32m    274\u001b[39m mask = indices < \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    276\u001b[39m     indices = indices.copy()\n\u001b[32m    277\u001b[39m     indices[mask] += n\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/numpy/_core/_methods.py:58\u001b[39m, in \u001b[36m_any\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     55\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_any\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m, *, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# By default, return a boolean for any and all\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     61\u001b[39m         dtype = bool_dt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Model Initialization\n",
    "\n",
    "model, vectorizer = train_optimized_model(training, master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using rank:pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Preparing training data...\n",
      "Reading data files...\n",
      "Training samples: 501\n",
      "Master products: 1000\n",
      "Found 500 matching SKUs\n",
      "Normalizing text...\n",
      "Initializing TF-IDF vectorizer...\n",
      "Creating ranking groups...\n",
      "Processing query 0/500\n",
      "Processing query 43400/500\n",
      "Processing query 44600/500\n",
      "Processing query 46600/500\n",
      "Created 500 ranking groups\n",
      "Total pairs: 3000\n",
      "Positive pairs: 500.0\n",
      "Negative pairs: 2500.0\n",
      "Splitting data...\n",
      "Training LightGBM model...\n",
      "Model saved as ranking_product_matcher_lgb.txt\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def compute_optimized_features_batch(pairs, tfidf_vectorizer):\n",
    "    \"\"\"Compute features for multiple pairs at once with vectorization\"\"\"\n",
    "    features = np.zeros((len(pairs), 4))\n",
    "    \n",
    "    texts1, texts2 = zip(*pairs)\n",
    "    \n",
    "    tfidf_vectors1 = tfidf_vectorizer.transform(texts1)\n",
    "    tfidf_vectors2 = tfidf_vectorizer.transform(texts2)\n",
    "    \n",
    "    cosine_sims = cosine_similarity(tfidf_vectors1, tfidf_vectors2)\n",
    "    \n",
    "    for i, (text1, text2) in enumerate(pairs):\n",
    "        features[i, 0] = fuzz.ratio(text1, text2)\n",
    "        features[i, 1] = fuzz.token_set_ratio(text1, text2)\n",
    "        \n",
    "        tokens1 = set(text1.split())\n",
    "        tokens2 = set(text2.split())\n",
    "        features[i, 2] = len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 or tokens2 else 0.0\n",
    "        features[i, 3] = cosine_sims[i, i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_ranking_training_data(train_file, master_file, neg_samples_per_query=5):\n",
    "    \"\"\"\n",
    "    Prepare training data for learning to rank approach with robust data validation\n",
    "    \"\"\"\n",
    "    print(\"Reading data files...\")\n",
    "    train_df = pd.read_excel(train_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file, usecols=['sku', 'product_name_ar'])\n",
    "    \n",
    "    # Remove any duplicates and null values\n",
    "    train_df = train_df.dropna(subset=['sku', 'seller_item_name']).drop_duplicates(subset=['sku'])\n",
    "    master_df = master_df.dropna(subset=['sku', 'product_name_ar']).drop_duplicates(subset=['sku'])\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Master products: {len(master_df)}\")\n",
    "    \n",
    "    # Find matching SKUs between training and master data\n",
    "    matching_skus = set(train_df['sku']).intersection(set(master_df['sku']))\n",
    "    print(f\"Found {len(matching_skus)} matching SKUs\")\n",
    "    \n",
    "    if len(matching_skus) == 0:\n",
    "        raise ValueError(\"No matching SKUs found between training and master data\")\n",
    "    \n",
    "    # Filter to only matching SKUs\n",
    "    train_df = train_df[train_df['sku'].isin(matching_skus)]\n",
    "    \n",
    "    print(\"Normalizing text...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, train_df['seller_item_name']\n",
    "        ))\n",
    "        master_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, master_df['product_name_ar']\n",
    "        ))\n",
    "    \n",
    "    print(\"Initializing TF-IDF vectorizer...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=2,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    all_texts = pd.concat([train_df['Normalized_Product'], master_df['Normalized_Product']]).unique()\n",
    "    tfidf_vectorizer.fit(all_texts)\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "        pickle.dump(tfidf_vectorizer, file)\n",
    "    \n",
    "    print(\"Creating ranking groups...\")\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    groups_list = []\n",
    "    query_ids = []\n",
    "    \n",
    "    # Create a master product lookup dictionary for efficiency\n",
    "    master_lookup = master_df.set_index('sku')['Normalized_Product'].to_dict()\n",
    "    \n",
    "    for idx, query_row in train_df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing query {idx}/{len(train_df)}\")\n",
    "            \n",
    "        query_sku = query_row['sku']\n",
    "        query_text = query_row['Normalized_Product']\n",
    "        \n",
    "        # Get positive match from lookup dictionary\n",
    "        positive_match = master_lookup[query_sku]\n",
    "        \n",
    "        # Sample negative matches from products with different SKUs\n",
    "        negative_skus = np.random.choice(\n",
    "            [sku for sku in master_lookup.keys() if sku != query_sku],\n",
    "            size=min(neg_samples_per_query, len(master_lookup)-1),\n",
    "            replace=False\n",
    "        )\n",
    "        negative_matches = [master_lookup[sku] for sku in negative_skus]\n",
    "        \n",
    "        # Create pairs for this query\n",
    "        all_pairs = [(query_text, positive_match)] + [(query_text, neg) for neg in negative_matches]\n",
    "        \n",
    "        # Compute features for all pairs in this group\n",
    "        group_features = compute_optimized_features_batch(all_pairs, tfidf_vectorizer)\n",
    "        \n",
    "        # Create labels (1 for positive match, 0 for negative matches)\n",
    "        group_labels = np.zeros(len(all_pairs))\n",
    "        group_labels[0] = 1\n",
    "        \n",
    "        features_list.append(group_features)\n",
    "        labels_list.append(group_labels)\n",
    "        groups_list.append(len(all_pairs))\n",
    "        query_ids.extend([idx] * len(all_pairs))  # Assign same query ID to all pairs in group\n",
    "    \n",
    "    if not features_list:\n",
    "        raise ValueError(\"No valid training pairs could be created\")\n",
    "    \n",
    "    X = np.vstack(features_list)\n",
    "    y = np.concatenate(labels_list)\n",
    "    groups = np.array(groups_list)\n",
    "    qids = np.array(query_ids)\n",
    "    \n",
    "    print(f\"Created {len(groups)} ranking groups\")\n",
    "    print(f\"Total pairs: {len(X)}\")\n",
    "    print(f\"Positive pairs: {sum(y)}\")\n",
    "    print(f\"Negative pairs: {len(y) - sum(y)}\")\n",
    "    \n",
    "    return X, y, groups, qids, tfidf_vectorizer\n",
    "\n",
    "def train_ranking_model(train_file, master_file):\n",
    "    \"\"\"Train a ranking-based model using LightGBM's ranking objective\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, groups, qids, tfidf_vectorizer = prepare_ranking_training_data(train_file, master_file)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    unique_qids = np.unique(qids)\n",
    "    n_queries = len(unique_qids)\n",
    "    train_query_idx = np.random.choice(n_queries, int(0.8 * n_queries), replace=False)\n",
    "    train_queries = unique_qids[train_query_idx]\n",
    "    \n",
    "    train_mask = np.isin(qids, train_queries)\n",
    "    val_mask = ~train_mask\n",
    "    \n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    qids_train = qids[train_mask]\n",
    "    qids_val = qids[val_mask]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train, group=[sum(qids_train == qid) for qid in np.unique(qids_train)])\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val, group=[sum(qids_val == qid) for qid in np.unique(qids_val)], reference=train_data)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': ['ndcg@5', 'map@5'],\n",
    "        'boosting': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 50,\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    print(\"Training LightGBM model...\")\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_data, valid_data],\n",
    "        valid_names=['train', 'valid']\n",
    "    )\n",
    "    \n",
    "    model.save_model(\"ranking_product_matcher_lgb.txt\")\n",
    "    print(\"Model saved as ranking_product_matcher_lgb.txt\")\n",
    "    \n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "model, vectorizer = train_ranking_model(training, master)\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect if text is primarily Arabic or English based on character count\n",
    "    Returns 'arabic' if primarily Arabic, 'english' if primarily English\n",
    "    \"\"\"\n",
    "    \n",
    "    # If more than 3 English letters, consider it English\n",
    "    if english_count > 3:\n",
    "        return 'english'\n",
    "    return 'arabic'\n",
    "\n",
    "def normalize_english(text):\n",
    "    \"\"\"Normalize English text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common product-related words and everything after them\n",
    "    text = re.sub(r'\\b(?:price|new|old|p n|p o)\\b.*', '', text)\n",
    "    \n",
    "    # Remove repeated letters (e.g., 'goood' -> 'good')\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def fast_normalize_arabic(text):\n",
    "    \"\"\"Extremely fast Arabic text normalization focusing only on critical operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#\\'()*+,.:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text based on detected language\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    language = detect_language(text)\n",
    "    if language == 'english':\n",
    "        return normalize_english(text)\n",
    "    return fast_normalize_arabic(text)\n",
    "\n",
    "def compute_token_overlap(text1, text2):\n",
    "    \"\"\"Compute token overlap between two texts\"\"\"\n",
    "    tokens1 = set(text1.split())\n",
    "    tokens2 = set(text2.split())\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    return len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "\n",
    "def precompute_tfidf_matrix(texts, vectorizer):\n",
    "    \"\"\"Precompute TF-IDF matrix for all texts\"\"\"\n",
    "    return vectorizer.transform(texts)\n",
    "\n",
    "def compute_batch_features(query_texts, master_texts, query_tfidf, master_tfidf):\n",
    "    \"\"\"Compute features for a batch of text pairs efficiently\"\"\"\n",
    "    # Calculate cosine similarities for the entire batch at once\n",
    "    cosine_sims = cosine_similarity(query_tfidf, master_tfidf)\n",
    "    \n",
    "    n_queries = len(query_texts)\n",
    "    n_masters = len(master_texts)\n",
    "    \n",
    "    # Initialize feature matrices\n",
    "    fuzz_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_set_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_overlaps = np.zeros((n_queries, n_masters))\n",
    "    \n",
    "    # Compute features in parallel\n",
    "    def compute_pair_features(i, j):\n",
    "        return (\n",
    "            fuzz.ratio(query_texts[i], master_texts[j]),\n",
    "            fuzz.token_set_ratio(query_texts[i], master_texts[j]),\n",
    "            compute_token_overlap(query_texts[i], master_texts[j])\n",
    "        )\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i in range(n_queries):\n",
    "            for j in range(n_masters):\n",
    "                futures.append(executor.submit(compute_pair_features, i, j))\n",
    "        \n",
    "        for idx, future in enumerate(futures):\n",
    "            i = idx // n_masters\n",
    "            j = idx % n_masters\n",
    "            ratio, token_set, overlap = future.result()\n",
    "            fuzz_ratios[i, j] = ratio\n",
    "            token_set_ratios[i, j] = token_set\n",
    "            token_overlaps[i, j] = overlap\n",
    "    \n",
    "    features = np.stack([\n",
    "        fuzz_ratios,\n",
    "        token_set_ratios,\n",
    "        token_overlaps,\n",
    "        cosine_sims\n",
    "    ], axis=2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def match_products_bilingual(query_file, master_file, model, vectorizer, threshold=0.5, output_file=\"MatchedResults_Pairwise.xlsx\"):\n",
    "    \"\"\"Bilingual matching function supporting both Arabic and English text\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    \n",
    "    # Load data efficiently\n",
    "    query_df = pd.read_excel(query_file, usecols=[ITEM_CODE, ITEM_NAME , ITEM_PRICE])\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    \n",
    "    print(f\"Loaded {len(query_df)} query products and {len(master_df)} master products\")\n",
    "    \n",
    "    # Detect language for each query product\n",
    "    print(\"Detecting languages and normalizing texts...\")\n",
    "    query_df['detected_language'] = query_df['product name'].apply(detect_language)\n",
    "    query_df['Normalized_Product'] = query_df['product name'].apply(normalize_text)\n",
    "    \n",
    "    # Initialize master product columns\n",
    "    master_df['Normalized_Product_AR'] = master_df['product_name_ar'].apply(fast_normalize_arabic)\n",
    "    master_df['Normalized_Product_EN'] = master_df['product_name'].apply(normalize_english)\n",
    "    \n",
    "    # Process Arabic and English queries separately\n",
    "    results = []\n",
    "    \n",
    "    # Handle Arabic queries\n",
    "    arabic_queries = query_df[query_df['detected_language'] == 'arabic']\n",
    "    if len(arabic_queries) > 0:\n",
    "        print(\"Processing Arabic queries...\")\n",
    "        arabic_query_tfidf = precompute_tfidf_matrix(arabic_queries['Normalized_Product'], vectorizer)\n",
    "        arabic_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_AR'], vectorizer)\n",
    "        \n",
    "        arabic_features = compute_batch_features(\n",
    "            arabic_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_AR'].tolist(),\n",
    "            arabic_query_tfidf,\n",
    "            arabic_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process Arabic predictions\n",
    "        arabic_predictions = process_predictions(\n",
    "            arabic_queries,\n",
    "            master_df,\n",
    "            arabic_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(arabic_predictions)\n",
    "    \n",
    "    # Handle English queries\n",
    "    english_queries = query_df[query_df['detected_language'] == 'english']\n",
    "    if len(english_queries) > 0:\n",
    "        print(\"Processing English queries...\")\n",
    "        english_query_tfidf = precompute_tfidf_matrix(english_queries['Normalized_Product'], vectorizer)\n",
    "        english_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_EN'], vectorizer)\n",
    "        \n",
    "        english_features = compute_batch_features(\n",
    "            english_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_EN'].tolist(),\n",
    "            english_query_tfidf,\n",
    "            english_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process English predictions\n",
    "        english_predictions = process_predictions(\n",
    "            english_queries,\n",
    "            master_df,\n",
    "            english_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(english_predictions)\n",
    "    \n",
    "    # Create and save results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"Matching completed in {processing_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(processing_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def process_predictions(query_df, master_df, features, model, threshold):\n",
    "    \"\"\"\n",
    "    Process predictions for a set of queries using XGBoost's predict method\n",
    "    \n",
    "    Parameters:\n",
    "    - query_df: DataFrame containing query products\n",
    "    - master_df: DataFrame containing master products\n",
    "    - features: Computed similarity features\n",
    "    - model: XGBoost model (Booster object)\n",
    "    - threshold: Minimum probability threshold for accepting matches\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing match results\n",
    "    \"\"\"\n",
    "    n_queries, n_masters, n_features = features.shape\n",
    "    features_reshaped = features.reshape(-1, n_features)\n",
    "    \n",
    "    # Convert features to DMatrix for XGBoost\n",
    "    dtest = xgb.DMatrix(features_reshaped)\n",
    "    \n",
    "    # Get raw predictions and convert to probabilities using softmax\n",
    "    predictions = model.predict(dtest)\n",
    "    \n",
    "    # If the model outputs raw scores (not probabilities), convert to probabilities\n",
    "    if len(predictions.shape) == 1:  # If predictions are 1-dimensional\n",
    "        predictions = 1 / (1 + np.exp(-predictions))  # Apply sigmoid for binary classification\n",
    "    else:  # If predictions are 2-dimensional (multiple classes)\n",
    "        predictions = predictions[:, 1]  # Take the probability of class 1\n",
    "    \n",
    "    predictions = predictions.reshape(n_queries, n_masters)\n",
    "    \n",
    "    best_match_indices = np.argmax(predictions, axis=1)\n",
    "    best_match_scores = np.max(predictions, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    for i, (_, query_row) in enumerate(query_df.iterrows()):\n",
    "        if best_match_scores[i] >= threshold:\n",
    "            master_row = master_df.iloc[best_match_indices[i]]\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row[ITEM_CODE],\n",
    "                \"Query Product\": query_row[ITEM_NAME],\n",
    "                \"Price\" : query_row[ITEM_PRICE],\n",
    "                \"Matched Master SKU\": master_row['sku'],\n",
    "                \"Matched Master Product\": master_row['product_name_ar'] if query_row['detected_language'] == 'arabic' else master_row['product_name'],\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row[ITEM_CODE],\n",
    "                \"Query Product\": query_row[ITEM_NAME],\n",
    "                \"Price\" : query_row[ITEM_PRICE],\n",
    "                \"Matched Master SKU\": None,\n",
    "                \"Matched Master Product\": None,\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_matching_with_timing(query_file, master_file, model, vectorizer , output_file=\"MatchedResults.xlsx\"):\n",
    "    \"\"\"Run matching with detailed timing information\"\"\"\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = match_products_bilingual(query_file, master_file, model, vectorizer, output_file=output_file)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(total_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM model and vectorizer loaded successfully.\n",
      "Starting bilingual matching process...\n",
      "Starting bilingual matching process...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './test.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m     vectorizer = pickle.load(file)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLightGBM model and vectorizer loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m results = \u001b[43mrun_matching_with_timing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMatchedResults_LightGBM.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 266\u001b[39m, in \u001b[36mrun_matching_with_timing\u001b[39m\u001b[34m(query_file, master_file, model, vectorizer, output_file)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting bilingual matching process...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    264\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m results = \u001b[43mmatch_products_bilingual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m end_time = time.time()\n\u001b[32m    269\u001b[39m total_time = end_time - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mmatch_products_bilingual\u001b[39m\u001b[34m(query_file, master_file, model, vectorizer, threshold, output_file)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting bilingual matching process...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Load data efficiently\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m query_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mITEM_CODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mITEM_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mITEM_PRICE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m master_df = pd.read_excel(master_file)\n\u001b[32m    128\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(query_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m query products and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(master_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m master products\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './test.xlsx'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "# Load the LightGBM ranking model\n",
    "model = lgb.Booster(model_file='ranking_product_matcher_lgb.txt')\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "print(\"LightGBM model and vectorizer loaded successfully.\")\n",
    "\n",
    "results = run_matching_with_timing(\n",
    "    test,\n",
    "    master,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    output_file=\"MatchedResults_LightGBM.xlsx\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Load the Logistic XGBoost model\n",
    "model = xgb.Booster()\n",
    "model.load_model('logistic_boosting_model.json')\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully.\")\n",
    "results = run_matching_with_timing(\n",
    "    test,\n",
    "    master,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    output_file=\"MatchedResults_Logistic.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
