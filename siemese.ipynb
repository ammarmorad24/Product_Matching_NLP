{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyarabic import araby\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize Arabic text by applying various preprocessing steps.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Apply Arabic-specific normalizations\n",
    "    text = araby.normalize_hamza(text)    # Normalize different forms of hamza\n",
    "    text = araby.normalize_ligature(text)  # Normalize ligatures\n",
    "    text = araby.normalize_alef(text)      # Convert أ إ آ to ا\n",
    "    text = araby.normalize_teh(text)       # Convert ة to ه\n",
    "    \n",
    "    # Remove punctuation and extra spaces\n",
    "    text = re.sub(r'[!\"#%\\'()*+,./:;<=>?@[\\\\]^_`{|}~]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function for Siamese networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, distance, label):\n",
    "        loss = (1 - label) * torch.pow(distance, 2) + \\\n",
    "               label * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class ProductMatchingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for product matching that prepares text pairs and their labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, model):\n",
    "        self.model = model\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Normalize text in both columns\n",
    "        df['Normalized Master Name'] = df['marketplace_product_name_ar'].apply(normalize_text)\n",
    "        df['Normalized Seller Name'] = df['seller_item_name'].apply(normalize_text)\n",
    "        \n",
    "        # Create embeddings for all texts\n",
    "        master_embeddings = self.encode_texts(df['Normalized Master Name'].tolist())\n",
    "        seller_embeddings = self.encode_texts(df['Normalized Seller Name'].tolist())\n",
    "        \n",
    "        # Create pairs and labels\n",
    "        for emb1, emb2, label in zip(master_embeddings, seller_embeddings, df['label']):\n",
    "            self.pairs.append((emb1, emb2))\n",
    "            self.labels.append(label)\n",
    "    \n",
    "    def encode_texts(self, texts):\n",
    "        return np.array(self.model.encode([str(t) for t in texts], convert_to_numpy=True))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        emb1, emb2 = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(emb1, dtype=torch.float32), \\\n",
    "               torch.tensor(emb2, dtype=torch.float32), \\\n",
    "               torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "class ImprovedSiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved Siamese network with better architecture for product matching.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ImprovedSiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Projection layers with dropout and batch normalization\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        return F.pairwise_distance(out1, out2)\n",
    "\n",
    "def train_improved_model(training_file, model, epochs=10, batch_size=64, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train the improved Siamese network with various enhancements.\n",
    "    Returns the best performing model based on validation loss.\n",
    "    \"\"\"\n",
    "    # Load and split data\n",
    "    df = pd.read_excel(training_file)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = ProductMatchingDataset(train_df, model)\n",
    "    val_dataset = ProductMatchingDataset(val_df, model)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    embedding_dim = model.get_sentence_embedding_dimension()\n",
    "    siamese_model = ImprovedSiameseNetwork(embedding_dim)\n",
    "    best_model = ImprovedSiameseNetwork(embedding_dim)  # Keep track of best model\n",
    "    criterion = ContrastiveLoss(margin=1.0)\n",
    "    optimizer = torch.optim.AdamW(siamese_model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        siamese_model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for emb1, emb2, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            distances = siamese_model(emb1, emb2)\n",
    "            loss = criterion(distances, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(siamese_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        siamese_model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for emb1, emb2, labels in val_loader:\n",
    "                distances = siamese_model(emb1, emb2)\n",
    "                val_loss = criterion(distances, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Update best model if current model is better\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model.load_state_dict(siamese_model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    return best_model\n",
    "def match_dataset_improved(master_file, dataset_file, output_file, model, siamese_model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match products between master and dataset files using the trained model.\n",
    "    Now includes proper index handling and tensor conversion.\n",
    "    \"\"\"\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    dataset_df = pd.read_excel(dataset_file)\n",
    "    \n",
    "    # Create temporary indices to ensure proper referencing\n",
    "    master_df = master_df.reset_index(drop=True)\n",
    "    dataset_df = dataset_df.reset_index(drop=True)\n",
    "    \n",
    "    # Normalize text\n",
    "    master_df['Normalized Name'] = master_df['product_name_ar'].apply(normalize_text)\n",
    "    dataset_df['Normalized Name'] = dataset_df['product_name_ar'].apply(normalize_text)\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for master products...\")\n",
    "    master_embeddings = model.encode(master_df['Normalized Name'].tolist(), convert_to_numpy=True)\n",
    "    print(\"Creating embeddings for dataset products...\")\n",
    "    dataset_embeddings = model.encode(dataset_df['Normalized Name'].tolist(), convert_to_numpy=True)\n",
    "    \n",
    "    results = []\n",
    "    siamese_model.eval()\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(f\"Processing {len(dataset_df)} products in batches of {batch_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset_df), batch_size):\n",
    "            current_batch_size = min(batch_size, len(dataset_df) - i)\n",
    "            batch_results = []\n",
    "            \n",
    "            # Get current batch of dataset embeddings\n",
    "            dataset_batch = torch.tensor(\n",
    "                dataset_embeddings[i:i + current_batch_size], \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            for j in range(0, len(master_df), batch_size):\n",
    "                current_master_batch_size = min(batch_size, len(master_df) - j)\n",
    "                \n",
    "                # Get current batch of master embeddings\n",
    "                master_batch = torch.tensor(\n",
    "                    master_embeddings[j:j + current_master_batch_size],\n",
    "                    dtype=torch.float32\n",
    "                )\n",
    "                \n",
    "                # Create all pairs between batches\n",
    "                dataset_expanded = dataset_batch.unsqueeze(1).expand(\n",
    "                    -1, current_master_batch_size, -1\n",
    "                )\n",
    "                master_expanded = master_batch.unsqueeze(0).expand(\n",
    "                    current_batch_size, -1, -1\n",
    "                )\n",
    "                \n",
    "                # Reshape for the model\n",
    "                dataset_flat = dataset_expanded.reshape(-1, dataset_expanded.size(-1))\n",
    "                master_flat = master_expanded.reshape(-1, master_expanded.size(-1))\n",
    "                \n",
    "                # Get distances\n",
    "                distances = siamese_model(dataset_flat, master_flat)\n",
    "                distances = distances.reshape(current_batch_size, current_master_batch_size)\n",
    "                \n",
    "                batch_results.append(distances)\n",
    "            \n",
    "            # Combine all batch results\n",
    "            all_distances = torch.cat(batch_results, dim=1)\n",
    "            \n",
    "            # Find best matches\n",
    "            min_distances, min_indices = torch.min(all_distances, dim=1)\n",
    "            \n",
    "            # Convert PyTorch tensors to numpy arrays for proper indexing\n",
    "            min_distances = min_distances.cpu().numpy()\n",
    "            min_indices = min_indices.cpu().numpy()\n",
    "            \n",
    "            # Process results for current batch\n",
    "            for k in range(current_batch_size):\n",
    "                dataset_idx = i + k\n",
    "                master_idx = min_indices[k]\n",
    "                \n",
    "                # Ensure indices are within bounds\n",
    "                if dataset_idx < len(dataset_df) and master_idx < len(master_df):\n",
    "                    results.append({\n",
    "                        'Seller Item': dataset_df.loc[dataset_idx, 'product_name_ar'],\n",
    "                        'Matched Item': master_df.loc[master_idx, 'product_name_ar'],\n",
    "                        'sku': master_df.loc[master_idx, 'sku'],\n",
    "                        'Match Score': 1 - min_distances[k],\n",
    "                        'Confidence': 'High' if min_distances[k] < threshold else 'Low'\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping invalid indices - dataset_idx: {dataset_idx}, master_idx: {master_idx}\")\n",
    "    \n",
    "    print(\"Creating results DataFrame...\")\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"Saving results to {output_file}...\")\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    print(f\"Matching completed. Output saved to {output_file}\")\n",
    "    \n",
    "    # Print matching statistics\n",
    "    print(\"\\nMatching Statistics:\")\n",
    "    print(f\"Total products processed: {len(dataset_df)}\")\n",
    "    print(f\"Total matches found: {len(results)}\")\n",
    "    print(f\"High confidence matches: {len(result_df[result_df['Confidence'] == 'High'])}\")\n",
    "    print(f\"Low confidence matches: {len(result_df[result_df['Confidence'] == 'Low'])}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load the multilingual BERT model\n",
    "        model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        \n",
    "        # Define file paths\n",
    "        training_file = \"/kaggle/input/augmented-dataset/PreDataset.xlsx\"\n",
    "        master_file = \"/kaggle/input/product-matching-dataset/Masterfile.xlsx\"\n",
    "        test_file = \"/kaggle/input/test-set-data/augmented_test_set.xlsx\"\n",
    "        output_file = \"/kaggle/working/results.xlsx\"\n",
    "        \n",
    "        # Train the model\n",
    "        siamese_model = train_improved_model(training_file, model)\n",
    "        \n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_dataset_improved(master_file, dataset_file, output_file, model, siamese_model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match products between master and dataset files using the trained model.\n",
    "    Includes accuracy calculation by comparing predicted SKUs with true SKUs.\n",
    "    \"\"\"\n",
    "    # Load and prepare the dataframes\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    dataset_df = pd.read_excel(dataset_file)\n",
    "    \n",
    "    # Create temporary indices to ensure proper referencing\n",
    "    master_df = master_df.reset_index(drop=True)\n",
    "    dataset_df = dataset_df.reset_index(drop=True)\n",
    "    \n",
    "    # Normalize text for better matching\n",
    "    master_df['Normalized Name'] = master_df['product_name_ar'].apply(normalize_text)\n",
    "    dataset_df['Normalized Name'] = dataset_df['product_name_ar'].apply(normalize_text)\n",
    "    \n",
    "    # Create embeddings using the BERT model\n",
    "    print(\"Creating embeddings for master products...\")\n",
    "    master_embeddings = model.encode(master_df['Normalized Name'].tolist(), convert_to_numpy=True)\n",
    "    print(\"Creating embeddings for dataset products...\")\n",
    "    dataset_embeddings = model.encode(dataset_df['Normalized Name'].tolist(), convert_to_numpy=True)\n",
    "    \n",
    "    results = []\n",
    "    siamese_model.eval()\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(f\"Processing {len(dataset_df)} products in batches of {batch_size}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset_df), batch_size):\n",
    "            current_batch_size = min(batch_size, len(dataset_df) - i)\n",
    "            batch_results = []\n",
    "            \n",
    "            # Process current batch of dataset embeddings\n",
    "            dataset_batch = torch.tensor(\n",
    "                dataset_embeddings[i:i + current_batch_size], \n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "            for j in range(0, len(master_df), batch_size):\n",
    "                current_master_batch_size = min(batch_size, len(master_df) - j)\n",
    "                \n",
    "                # Process current batch of master embeddings\n",
    "                master_batch = torch.tensor(\n",
    "                    master_embeddings[j:j + current_master_batch_size],\n",
    "                    dtype=torch.float32\n",
    "                )\n",
    "                \n",
    "                # Create pairs between batches for comparison\n",
    "                dataset_expanded = dataset_batch.unsqueeze(1).expand(\n",
    "                    -1, current_master_batch_size, -1\n",
    "                )\n",
    "                master_expanded = master_batch.unsqueeze(0).expand(\n",
    "                    current_batch_size, -1, -1\n",
    "                )\n",
    "                \n",
    "                # Prepare data for the model\n",
    "                dataset_flat = dataset_expanded.reshape(-1, dataset_expanded.size(-1))\n",
    "                master_flat = master_expanded.reshape(-1, master_expanded.size(-1))\n",
    "                \n",
    "                # Calculate distances using the Siamese model\n",
    "                distances = siamese_model(dataset_flat, master_flat)\n",
    "                distances = distances.reshape(current_batch_size, current_master_batch_size)\n",
    "                \n",
    "                batch_results.append(distances)\n",
    "            \n",
    "            # Combine results and find best matches\n",
    "            all_distances = torch.cat(batch_results, dim=1)\n",
    "            min_distances, min_indices = torch.min(all_distances, dim=1)\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            min_distances = min_distances.cpu().numpy()\n",
    "            min_indices = min_indices.cpu().numpy()\n",
    "            \n",
    "            # Process results for the current batch\n",
    "            for k in range(current_batch_size):\n",
    "                dataset_idx = i + k\n",
    "                master_idx = min_indices[k]\n",
    "                \n",
    "                # Ensure indices are within bounds\n",
    "                if dataset_idx < len(dataset_df) and master_idx < len(master_df):\n",
    "                    results.append({\n",
    "                        'Seller Item': dataset_df.loc[dataset_idx, 'product_name_ar'],\n",
    "                        'Matched Item': master_df.loc[master_idx, 'product_name_ar'],\n",
    "                        'Predicted SKU': master_df.loc[master_idx, 'sku'],\n",
    "                        'True SKU': dataset_df.loc[dataset_idx, 'sku'],\n",
    "                        'Match Score': 1 - min_distances[k],\n",
    "                        'Confidence': 'High' if min_distances[k] < threshold else 'Low'\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping invalid indices - dataset_idx: {dataset_idx}, master_idx: {master_idx}\")\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    print(\"Creating results DataFrame...\")\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate accuracy metrics\n",
    "    correct_matches = result_df['Predicted SKU'] == result_df['True SKU']\n",
    "    overall_accuracy = correct_matches.mean() * 100\n",
    "    \n",
    "    # Calculate accuracy for high confidence matches\n",
    "    high_conf_mask = result_df['Confidence'] == 'High'\n",
    "    high_conf_accuracy = (\n",
    "        result_df[high_conf_mask]['Predicted SKU'] == \n",
    "        result_df[high_conf_mask]['True SKU']\n",
    "    ).mean() * 100 if high_conf_mask.any() else 0\n",
    "    \n",
    "    # Add accuracy metrics to the output\n",
    "    print(\"\\nMatching Statistics:\")\n",
    "    print(f\"Total products processed: {len(dataset_df)}\")\n",
    "    print(f\"Total matches found: {len(results)}\")\n",
    "    print(f\"Overall accuracy: {overall_accuracy:.2f}%\")\n",
    "    print(f\"High confidence matches: {len(result_df[result_df['Confidence'] == 'High'])}\")\n",
    "    print(f\"High confidence accuracy: {high_conf_accuracy:.2f}%\")\n",
    "    print(f\"Low confidence matches: {len(result_df[result_df['Confidence'] == 'Low'])}\")\n",
    "    \n",
    "    # Save results with accuracy information\n",
    "    print(f\"\\nSaving results to {output_file}...\")\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        result_df.to_excel(writer, sheet_name='Matches', index=False)\n",
    "        \n",
    "        # Create a summary sheet with accuracy metrics\n",
    "        summary_data = {\n",
    "            'Metric': ['Total Products', 'Total Matches', 'Overall Accuracy', \n",
    "                      'High Confidence Matches', 'High Confidence Accuracy',\n",
    "                      'Low Confidence Matches'],\n",
    "            'Value': [len(dataset_df), len(results), f\"{overall_accuracy:.2f}%\",\n",
    "                     len(result_df[high_conf_mask]), f\"{high_conf_accuracy:.2f}%\",\n",
    "                     len(result_df[~high_conf_mask])]\n",
    "        }\n",
    "        pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    print(\"Matching completed. Results and summary saved to Excel file.\")\n",
    "    \n",
    "    return overall_accuracy, high_conf_accuracy\n",
    "match_dataset_improved(master_file, test_file, output_file, model, siamese_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
