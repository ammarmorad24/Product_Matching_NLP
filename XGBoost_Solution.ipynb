{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==3.0.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: comm==0.2.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: debugpy==1.8.11 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.8.11)\n",
      "Requirement already satisfied: decorator==5.2.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (5.2.1)\n",
      "Requirement already satisfied: et_xmlfile==2.0.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.1.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: fuzzywuzzy==0.18.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: importlib_metadata==8.6.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (8.6.1)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (6.29.5)\n",
      "Requirement already satisfied: ipython==9.0.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (9.0.2)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.19.2)\n",
      "Requirement already satisfied: joblib==1.4.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (1.4.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (5.7.2)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (0.27.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (0.1.7)\n",
      "Requirement already satisfied: nest_asyncio==1.6.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.2.4 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (2.2.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (2.26.2)\n",
      "Requirement already satisfied: openpyxl==3.1.5 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (3.1.5)\n",
      "Requirement already satisfied: packaging==24.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (24.2)\n",
      "Requirement already satisfied: pandas==2.2.3 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (2.2.3)\n",
      "Requirement already satisfied: parso==0.8.4 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (4.9.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 27)) (0.7.5)\n",
      "Requirement already satisfied: pip==25.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (25.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 29)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.50 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 30)) (3.0.50)\n",
      "Requirement already satisfied: psutil==5.9.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 31)) (5.9.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 32)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 33)) (0.2.3)\n",
      "Requirement already satisfied: PyArabic==0.6.15 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 34)) (0.6.15)\n",
      "Requirement already satisfied: Pygments==2.19.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 35)) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 36)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-Levenshtein==0.27.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 37)) (0.27.1)\n",
      "Requirement already satisfied: pytz==2025.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 38)) (2025.1)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 39)) (26.2.0)\n",
      "Requirement already satisfied: RapidFuzz==3.12.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 40)) (3.12.2)\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 41)) (1.6.1)\n",
      "Requirement already satisfied: scipy==1.15.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 42)) (1.15.2)\n",
      "Requirement already satisfied: setuptools==75.8.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 43)) (75.8.0)\n",
      "Requirement already satisfied: six==1.17.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 44)) (1.17.0)\n",
      "Requirement already satisfied: stack_data==0.6.3 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 45)) (0.6.3)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 46)) (3.6.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 47)) (6.4.2)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 48)) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 49)) (4.12.2)\n",
      "Requirement already satisfied: tzdata==2025.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 50)) (2025.1)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 51)) (0.2.13)\n",
      "Requirement already satisfied: wheel==0.45.1 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 52)) (0.45.1)\n",
      "Requirement already satisfied: xgboost==3.0.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 53)) (3.0.0)\n",
      "Requirement already satisfied: zipp==3.21.0 in /home/ammarmorad/anaconda3/envs/test/lib/python3.12/site-packages (from -r requirements.txt (line 54)) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from pyarabic import araby\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables for Easy Accessibility\n",
    "\n",
    "* **master**: Path to the master file containing the correct names.\n",
    "* **training**: Path to the desired training data.\n",
    "* **test**: File to match data with and test the model.\n",
    "\n",
    "### Column Requirements\n",
    "\n",
    "- **Master File**: Columns should be in the format `(sku, product_name_ar, product_name)`.\n",
    "- **Training File**: Columns should be in the format `(sku, ..., seller_item_name)`.\n",
    "- **Test File**: Columns should be in the format `(item code , product name , price)`.\n",
    "  added a dynamic naming for the test file\n",
    "\n",
    "The model uses the `sku` in the master file to find matches, so only the `sku` is needed in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"./Masterfile.xlsx\"\n",
    "training = \"./Dedup_Dataset.xlsx\"\n",
    "test = \"./test.xlsx\"\n",
    "\n",
    "# dynamic test column names\n",
    "ITEM_NAME = \"product name\"\n",
    "ITEM_CODE = \"item code\"\n",
    "ITEM_PRICE = \"price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Preprocessing\n",
    "\n",
    "The two functions provided serve the same purpose. The first function is written in a more readable format to illustrate the approach taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    \"\"\"Harsh and Kinda extreme preprocessing but for the clarity\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#\\'()*+,.:;<=>?@[\\\\]^_`{|}~]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)  # Remove specific words and everything after them\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)  # Remove specific words even if they are part of another word\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)  # Remove repeated letters\n",
    "    return text\n",
    "\n",
    "\n",
    "def optimize_normalize_arabic(text):\n",
    "    \"\"\"Enhanced Arabic text normalization with reduced operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = araby.normalize_hamza(araby.normalize_ligature(araby.normalize_alef(araby.normalize_teh(text))))\n",
    "    text = re.sub(\n",
    "        r'[^\\w\\s]|(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*|(سعر|جديد|قديم|س ق|س ج)|(.)\\1+',\n",
    "        lambda m: m.group(2) if m.group(2) else '',\n",
    "        text\n",
    "    )\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and its Features\n",
    "## 1. using Binary Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimized_features_batch(pairs, tfidf_vectorizer):\n",
    "    \"\"\"Compute features for multiple pairs at once with corrected vectorization\"\"\"\n",
    "    features = np.zeros((len(pairs), 4))\n",
    "    \n",
    "    # Extract text pairs\n",
    "    texts1, texts2 = zip(*pairs)\n",
    "    \n",
    "    # Compute TF-IDF vectors for all texts at once\n",
    "    tfidf_vectors1 = tfidf_vectorizer.transform(texts1)\n",
    "    tfidf_vectors2 = tfidf_vectorizer.transform(texts2)\n",
    "    \n",
    "    # Compute cosine similarities in one go\n",
    "    cosine_sims = cosine_similarity(tfidf_vectors1, tfidf_vectors2)\n",
    "    \n",
    "    for i, (text1, text2) in enumerate(pairs):\n",
    "        # Fuzzy string matching features\n",
    "        features[i, 0] = fuzz.ratio(text1, text2)\n",
    "        features[i, 1] = fuzz.token_set_ratio(text1, text2)\n",
    "        \n",
    "        # Token overlap\n",
    "        tokens1 = set(text1.split())\n",
    "        tokens2 = set(text2.split())\n",
    "        features[i, 2] = len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 or tokens2 else 0.0\n",
    "        \n",
    "        # TF-IDF cosine similarity\n",
    "        features[i, 3] = cosine_sims[i, i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_optimized_training_data(train_file, master_file, sample_negatives=2):\n",
    "    \"\"\"Optimized training data preparation with corrected batch processing\"\"\"\n",
    "    # Read data efficiently\n",
    "    print(\"Reading data files...\")\n",
    "    train_df = pd.read_excel(train_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file, usecols=['sku', 'product_name_ar'])\n",
    "    \n",
    "    # Parallel text normalization\n",
    "    print(\"Normalizing text...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, train_df['seller_item_name']\n",
    "        ))\n",
    "        master_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, master_df['product_name_ar']\n",
    "        ))\n",
    "    \n",
    "    # Create positive pairs using vectorized operations\n",
    "    print(\"Creating positive pairs...\")\n",
    "    train_master_merged = train_df.merge(\n",
    "        master_df, on='sku', suffixes=('_train', '_master')\n",
    "    )\n",
    "    pos_pairs = list(zip(\n",
    "        train_master_merged['Normalized_Product_train'],\n",
    "        train_master_merged['Normalized_Product_master']\n",
    "    ))\n",
    "    \n",
    "    # Create negative pairs efficiently\n",
    "    print(\"Creating negative pairs...\")\n",
    "    neg_pairs = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        negative_samples = master_df[master_df['sku'] != row['sku']].sample(\n",
    "            n=min(sample_negatives, len(master_df)-1),\n",
    "            random_state=42\n",
    "        )\n",
    "        neg_pairs.extend([\n",
    "            (row['Normalized_Product'], neg_row['Normalized_Product'])\n",
    "            for _, neg_row in negative_samples.iterrows()\n",
    "        ])\n",
    "    \n",
    "    # Combine all pairs\n",
    "    all_pairs = pos_pairs + neg_pairs\n",
    "    \n",
    "    # Prepare TF-IDF vectorizer\n",
    "    print(\"Computing TF-IDF features...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=2,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    # Fit vectorizer on all texts\n",
    "    all_texts = [text for pair in all_pairs for text in pair]\n",
    "    tfidf_vectorizer.fit(all_texts)\n",
    "    \n",
    "    # Process features in batches\n",
    "    print(\"Computing features in batches...\")\n",
    "    batch_size = 1000\n",
    "    X = []\n",
    "    for i in range(0, len(all_pairs), batch_size):\n",
    "        batch_pairs = all_pairs[i:i+batch_size]\n",
    "        batch_features = compute_optimized_features_batch(\n",
    "            batch_pairs,\n",
    "            tfidf_vectorizer\n",
    "        )\n",
    "        X.append(batch_features)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate([\n",
    "        np.ones(len(pos_pairs)),\n",
    "        np.zeros(len(neg_pairs))\n",
    "    ])\n",
    "    \n",
    "    return X, y, tfidf_vectorizer\n",
    "\n",
    "def train_optimized_model(train_file, master_file):\n",
    "    \"\"\"Train model with optimized parameters and early stopping\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, tfidf_vectorizer = prepare_optimized_training_data(train_file, master_file)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to DMatrix for faster training\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model(\"logistic_boosting_model.json\")\n",
    "    print(\"Model saved as logistic_boosting_model.json\")\n",
    "    \n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Reading data files...\n",
      "Normalizing text...\n",
      "Creating positive pairs...\n",
      "Creating negative pairs...\n",
      "Computing TF-IDF features...\n",
      "Computing features in batches...\n",
      "Splitting data...\n",
      "Training model...\n",
      "[0]\ttrain-logloss:0.08100\tval-logloss:0.08081\n",
      "[100]\ttrain-logloss:0.00844\tval-logloss:0.00901\n",
      "[200]\ttrain-logloss:0.00780\tval-logloss:0.00852\n",
      "[300]\ttrain-logloss:0.00754\tval-logloss:0.00835\n",
      "[400]\ttrain-logloss:0.00737\tval-logloss:0.00825\n",
      "[500]\ttrain-logloss:0.00725\tval-logloss:0.00818\n",
      "[600]\ttrain-logloss:0.00715\tval-logloss:0.00813\n",
      "[609]\ttrain-logloss:0.00715\tval-logloss:0.00814\n",
      "Model saved as logistic_boosting_model.json\n"
     ]
    }
   ],
   "source": [
    "#Model Initialization\n",
    "\n",
    "model, vectorizer = train_optimized_model(training, master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using rank:pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Preparing training data...\n",
      "Reading data files...\n",
      "Training samples: 501\n",
      "Master products: 1000\n",
      "Found 500 matching SKUs\n",
      "Normalizing text...\n",
      "Initializing TF-IDF vectorizer...\n",
      "Creating ranking groups...\n",
      "Processing query 0/500\n",
      "Processing query 43400/500\n",
      "Processing query 44600/500\n",
      "Processing query 46600/500\n",
      "Created 500 ranking groups\n",
      "Total pairs: 3000\n",
      "Positive pairs: 500.0\n",
      "Negative pairs: 2500.0\n",
      "Splitting data...\n",
      "Training model...\n",
      "[0]\ttrain-ndcg@5:0.99608\ttrain-map@5:0.99479\tval-ndcg@5:1.00000\tval-map@5:1.00000\n",
      "[9]\ttrain-ndcg@5:0.99765\ttrain-map@5:0.99687\tval-ndcg@5:0.99631\tval-map@5:0.99500\n",
      "Model saved as ranking_product_matcher.json\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "def compute_optimized_features_batch(pairs, tfidf_vectorizer):\n",
    "    \"\"\"Compute features for multiple pairs at once with vectorization\"\"\"\n",
    "    features = np.zeros((len(pairs), 4))\n",
    "    \n",
    "    texts1, texts2 = zip(*pairs)\n",
    "    \n",
    "    tfidf_vectors1 = tfidf_vectorizer.transform(texts1)\n",
    "    tfidf_vectors2 = tfidf_vectorizer.transform(texts2)\n",
    "    \n",
    "    cosine_sims = cosine_similarity(tfidf_vectors1, tfidf_vectors2)\n",
    "    \n",
    "    for i, (text1, text2) in enumerate(pairs):\n",
    "        features[i, 0] = fuzz.ratio(text1, text2)\n",
    "        features[i, 1] = fuzz.token_set_ratio(text1, text2)\n",
    "        \n",
    "        tokens1 = set(text1.split())\n",
    "        tokens2 = set(text2.split())\n",
    "        features[i, 2] = len(tokens1 & tokens2) / len(tokens1 | tokens2) if tokens1 or tokens2 else 0.0\n",
    "        features[i, 3] = cosine_sims[i, i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_ranking_training_data(train_file, master_file, neg_samples_per_query=5):\n",
    "    \"\"\"\n",
    "    Prepare training data for learning to rank approach with robust data validation\n",
    "    \"\"\"\n",
    "    print(\"Reading data files...\")\n",
    "    train_df = pd.read_excel(train_file, usecols=['sku', 'seller_item_name'])\n",
    "    master_df = pd.read_excel(master_file, usecols=['sku', 'product_name_ar'])\n",
    "    \n",
    "    # Remove any duplicates and null values\n",
    "    train_df = train_df.dropna(subset=['sku', 'seller_item_name']).drop_duplicates(subset=['sku'])\n",
    "    master_df = master_df.dropna(subset=['sku', 'product_name_ar']).drop_duplicates(subset=['sku'])\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Master products: {len(master_df)}\")\n",
    "    \n",
    "    # Find matching SKUs between training and master data\n",
    "    matching_skus = set(train_df['sku']).intersection(set(master_df['sku']))\n",
    "    print(f\"Found {len(matching_skus)} matching SKUs\")\n",
    "    \n",
    "    if len(matching_skus) == 0:\n",
    "        raise ValueError(\"No matching SKUs found between training and master data\")\n",
    "    \n",
    "    # Filter to only matching SKUs\n",
    "    train_df = train_df[train_df['sku'].isin(matching_skus)]\n",
    "    \n",
    "    print(\"Normalizing text...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, train_df['seller_item_name']\n",
    "        ))\n",
    "        master_df['Normalized_Product'] = list(executor.map(\n",
    "            optimize_normalize_arabic, master_df['product_name_ar']\n",
    "        ))\n",
    "    \n",
    "    print(\"Initializing TF-IDF vectorizer...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=3000,\n",
    "        min_df=2,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    all_texts = pd.concat([train_df['Normalized_Product'], master_df['Normalized_Product']]).unique()\n",
    "    tfidf_vectorizer.fit(all_texts)\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "        pickle.dump(tfidf_vectorizer, file)\n",
    "    \n",
    "    print(\"Creating ranking groups...\")\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    groups_list = []\n",
    "    \n",
    "    # Create a master product lookup dictionary for efficiency\n",
    "    master_lookup = master_df.set_index('sku')['Normalized_Product'].to_dict()\n",
    "    \n",
    "    for idx, query_row in train_df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing query {idx}/{len(train_df)}\")\n",
    "            \n",
    "        query_sku = query_row['sku']\n",
    "        query_text = query_row['Normalized_Product']\n",
    "        \n",
    "        # Get positive match from lookup dictionary\n",
    "        positive_match = master_lookup[query_sku]\n",
    "        \n",
    "        # Sample negative matches from products with different SKUs\n",
    "        negative_skus = np.random.choice(\n",
    "            [sku for sku in master_lookup.keys() if sku != query_sku],\n",
    "            size=min(neg_samples_per_query, len(master_lookup)-1),\n",
    "            replace=False\n",
    "        )\n",
    "        negative_matches = [master_lookup[sku] for sku in negative_skus]\n",
    "        \n",
    "        # Create pairs for this query\n",
    "        all_pairs = [(query_text, positive_match)] + [(query_text, neg) for neg in negative_matches]\n",
    "        \n",
    "        # Compute features for all pairs in this group\n",
    "        group_features = compute_optimized_features_batch(all_pairs, tfidf_vectorizer)\n",
    "        \n",
    "        # Create labels (1 for positive match, 0 for negative matches)\n",
    "        group_labels = np.zeros(len(all_pairs))\n",
    "        group_labels[0] = 1\n",
    "        \n",
    "        features_list.append(group_features)\n",
    "        labels_list.append(group_labels)\n",
    "        groups_list.append(len(all_pairs))\n",
    "    \n",
    "    if not features_list:\n",
    "        raise ValueError(\"No valid training pairs could be created\")\n",
    "    \n",
    "    X = np.vstack(features_list)\n",
    "    y = np.concatenate(labels_list)\n",
    "    groups = np.array(groups_list)\n",
    "    \n",
    "    print(f\"Created {len(groups)} ranking groups\")\n",
    "    print(f\"Total pairs: {len(X)}\")\n",
    "    print(f\"Positive pairs: {sum(y)}\")\n",
    "    print(f\"Negative pairs: {len(y) - sum(y)}\")\n",
    "    \n",
    "    return X, y, groups, tfidf_vectorizer\n",
    "\n",
    "def train_ranking_model(train_file, master_file):\n",
    "    \"\"\"Train a ranking-based model using XGBoost's ranking objective\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y, groups, tfidf_vectorizer = prepare_ranking_training_data(train_file, master_file)\n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    unique_groups = np.cumsum(groups)\n",
    "    n_groups = len(groups)\n",
    "    train_idx = np.random.choice(n_groups, int(0.8 * n_groups), replace=False)\n",
    "    val_idx = np.array(list(set(range(n_groups)) - set(train_idx)))\n",
    "    \n",
    "    train_mask = np.zeros(len(X), dtype=bool)\n",
    "    val_mask = np.zeros(len(X), dtype=bool)\n",
    "    \n",
    "    start = 0\n",
    "    for i in range(n_groups):\n",
    "        end = start + groups[i]\n",
    "        if i in train_idx:\n",
    "            train_mask[start:end] = True\n",
    "        else:\n",
    "            val_mask[start:end] = True\n",
    "        start = end\n",
    "    \n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_val, y_val = X[val_mask], y[val_mask]\n",
    "    groups_train = groups[train_idx]\n",
    "    groups_val = groups[val_idx]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtrain.set_group(groups_train)\n",
    "    dval.set_group(groups_val)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'rank:pairwise',\n",
    "        'eval_metric': ['ndcg@5', 'map@5'],\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    model.save_model(\"ranking_product_matcher.json\")\n",
    "    print(\"Model saved as ranking_product_matcher.json\")\n",
    "    \n",
    "    return model, tfidf_vectorizer\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "model, vectorizer = train_ranking_model(training, master)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detect if text is primarily Arabic or English based on character count\n",
    "    Returns 'arabic' if primarily Arabic, 'english' if primarily English\n",
    "    \"\"\"\n",
    "    # Count English letters (a-z, A-Z)\n",
    "    english_count = len(re.findall(r'[a-zA-Z]', text))\n",
    "    \n",
    "    # If more than 3 English letters, consider it English\n",
    "    if english_count > 3:\n",
    "        return 'english'\n",
    "    return 'arabic'\n",
    "\n",
    "def normalize_english(text):\n",
    "    \"\"\"Normalize English text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove common product-related words and everything after them\n",
    "    text = re.sub(r'\\b(?:price|new|old|p n|p o)\\b.*', '', text)\n",
    "    \n",
    "    # Remove repeated letters (e.g., 'goood' -> 'good')\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def fast_normalize_arabic(text):\n",
    "    \"\"\"Extremely fast Arabic text normalization focusing only on critical operations\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    text = araby.normalize_hamza(text)\n",
    "    text = araby.normalize_ligature(text)\n",
    "    text = araby.normalize_alef(text)\n",
    "    text = araby.normalize_teh(text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[!\"#\\'()*+,.:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    text = re.sub(r'\\b(?:سعر|جديد|قديم|س ق|س ج|س|ق|ج|س.ج|س.ق)\\b.*', '', text)\n",
    "    text = re.sub(r'(سعر|جديد|قديم|س ق|س ج)', '', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text based on detected language\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "        \n",
    "    language = detect_language(text)\n",
    "    if language == 'english':\n",
    "        return normalize_english(text)\n",
    "    return fast_normalize_arabic(text)\n",
    "\n",
    "def compute_token_overlap(text1, text2):\n",
    "    \"\"\"Compute token overlap between two texts\"\"\"\n",
    "    tokens1 = set(text1.split())\n",
    "    tokens2 = set(text2.split())\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    return len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "\n",
    "def precompute_tfidf_matrix(texts, vectorizer):\n",
    "    \"\"\"Precompute TF-IDF matrix for all texts\"\"\"\n",
    "    return vectorizer.transform(texts)\n",
    "\n",
    "def compute_batch_features(query_texts, master_texts, query_tfidf, master_tfidf):\n",
    "    \"\"\"Compute features for a batch of text pairs efficiently\"\"\"\n",
    "    # Calculate cosine similarities for the entire batch at once\n",
    "    cosine_sims = cosine_similarity(query_tfidf, master_tfidf)\n",
    "    \n",
    "    n_queries = len(query_texts)\n",
    "    n_masters = len(master_texts)\n",
    "    \n",
    "    # Initialize feature matrices\n",
    "    fuzz_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_set_ratios = np.zeros((n_queries, n_masters))\n",
    "    token_overlaps = np.zeros((n_queries, n_masters))\n",
    "    \n",
    "    # Compute features in parallel\n",
    "    def compute_pair_features(i, j):\n",
    "        return (\n",
    "            fuzz.ratio(query_texts[i], master_texts[j]),\n",
    "            fuzz.token_set_ratio(query_texts[i], master_texts[j]),\n",
    "            compute_token_overlap(query_texts[i], master_texts[j])\n",
    "        )\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = []\n",
    "        for i in range(n_queries):\n",
    "            for j in range(n_masters):\n",
    "                futures.append(executor.submit(compute_pair_features, i, j))\n",
    "        \n",
    "        for idx, future in enumerate(futures):\n",
    "            i = idx // n_masters\n",
    "            j = idx % n_masters\n",
    "            ratio, token_set, overlap = future.result()\n",
    "            fuzz_ratios[i, j] = ratio\n",
    "            token_set_ratios[i, j] = token_set\n",
    "            token_overlaps[i, j] = overlap\n",
    "    \n",
    "    features = np.stack([\n",
    "        fuzz_ratios,\n",
    "        token_set_ratios,\n",
    "        token_overlaps,\n",
    "        cosine_sims\n",
    "    ], axis=2)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def match_products_bilingual(query_file, master_file, model, vectorizer, threshold=0.5, output_file=\"MatchedResults_Pairwise.xlsx\"):\n",
    "    \"\"\"Bilingual matching function supporting both Arabic and English text\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    \n",
    "    # Load data efficiently\n",
    "    query_df = pd.read_excel(query_file, usecols=[ITEM_CODE, ITEM_NAME , ITEM_PRICE])\n",
    "    master_df = pd.read_excel(master_file)\n",
    "    \n",
    "    print(f\"Loaded {len(query_df)} query products and {len(master_df)} master products\")\n",
    "    \n",
    "    # Detect language for each query product\n",
    "    print(\"Detecting languages and normalizing texts...\")\n",
    "    query_df['detected_language'] = query_df['product name'].apply(detect_language)\n",
    "    query_df['Normalized_Product'] = query_df['product name'].apply(normalize_text)\n",
    "    \n",
    "    # Initialize master product columns\n",
    "    master_df['Normalized_Product_AR'] = master_df['product_name_ar'].apply(fast_normalize_arabic)\n",
    "    master_df['Normalized_Product_EN'] = master_df['product_name'].apply(normalize_english)\n",
    "    \n",
    "    # Process Arabic and English queries separately\n",
    "    results = []\n",
    "    \n",
    "    # Handle Arabic queries\n",
    "    arabic_queries = query_df[query_df['detected_language'] == 'arabic']\n",
    "    if len(arabic_queries) > 0:\n",
    "        print(\"Processing Arabic queries...\")\n",
    "        arabic_query_tfidf = precompute_tfidf_matrix(arabic_queries['Normalized_Product'], vectorizer)\n",
    "        arabic_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_AR'], vectorizer)\n",
    "        \n",
    "        arabic_features = compute_batch_features(\n",
    "            arabic_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_AR'].tolist(),\n",
    "            arabic_query_tfidf,\n",
    "            arabic_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process Arabic predictions\n",
    "        arabic_predictions = process_predictions(\n",
    "            arabic_queries,\n",
    "            master_df,\n",
    "            arabic_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(arabic_predictions)\n",
    "    \n",
    "    # Handle English queries\n",
    "    english_queries = query_df[query_df['detected_language'] == 'english']\n",
    "    if len(english_queries) > 0:\n",
    "        print(\"Processing English queries...\")\n",
    "        english_query_tfidf = precompute_tfidf_matrix(english_queries['Normalized_Product'], vectorizer)\n",
    "        english_master_tfidf = precompute_tfidf_matrix(master_df['Normalized_Product_EN'], vectorizer)\n",
    "        \n",
    "        english_features = compute_batch_features(\n",
    "            english_queries['Normalized_Product'].tolist(),\n",
    "            master_df['Normalized_Product_EN'].tolist(),\n",
    "            english_query_tfidf,\n",
    "            english_master_tfidf\n",
    "        )\n",
    "        \n",
    "        # Process English predictions\n",
    "        english_predictions = process_predictions(\n",
    "            english_queries,\n",
    "            master_df,\n",
    "            english_features,\n",
    "            model,\n",
    "            threshold\n",
    "        )\n",
    "        results.extend(english_predictions)\n",
    "    \n",
    "    # Create and save results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    print(f\"Matching completed in {processing_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(processing_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def process_predictions(query_df, master_df, features, model, threshold):\n",
    "    \"\"\"\n",
    "    Process predictions for a set of queries using XGBoost's predict method\n",
    "    \n",
    "    Parameters:\n",
    "    - query_df: DataFrame containing query products\n",
    "    - master_df: DataFrame containing master products\n",
    "    - features: Computed similarity features\n",
    "    - model: XGBoost model (Booster object)\n",
    "    - threshold: Minimum probability threshold for accepting matches\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries containing match results\n",
    "    \"\"\"\n",
    "    n_queries, n_masters, n_features = features.shape\n",
    "    features_reshaped = features.reshape(-1, n_features)\n",
    "    \n",
    "    # Convert features to DMatrix for XGBoost\n",
    "    dtest = xgb.DMatrix(features_reshaped)\n",
    "    \n",
    "    # Get raw predictions and convert to probabilities using softmax\n",
    "    predictions = model.predict(dtest)\n",
    "    \n",
    "    # If the model outputs raw scores (not probabilities), convert to probabilities\n",
    "    if len(predictions.shape) == 1:  # If predictions are 1-dimensional\n",
    "        predictions = 1 / (1 + np.exp(-predictions))  # Apply sigmoid for binary classification\n",
    "    else:  # If predictions are 2-dimensional (multiple classes)\n",
    "        predictions = predictions[:, 1]  # Take the probability of class 1\n",
    "    \n",
    "    predictions = predictions.reshape(n_queries, n_masters)\n",
    "    \n",
    "    best_match_indices = np.argmax(predictions, axis=1)\n",
    "    best_match_scores = np.max(predictions, axis=1)\n",
    "    \n",
    "    results = []\n",
    "    for i, (_, query_row) in enumerate(query_df.iterrows()):\n",
    "        if best_match_scores[i] >= threshold:\n",
    "            master_row = master_df.iloc[best_match_indices[i]]\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row[ITEM_CODE],\n",
    "                \"Query Product\": query_row[ITEM_NAME],\n",
    "                \"Price\" : query_row[ITEM_PRICE],\n",
    "                \"Matched Master SKU\": master_row['sku'],\n",
    "                \"Matched Master Product\": master_row['product_name_ar'] if query_row['detected_language'] == 'arabic' else master_row['product_name'],\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                \"Query SKU\": query_row[ITEM_CODE],\n",
    "                \"Query Product\": query_row[ITEM_NAME],\n",
    "                \"Price\" : query_row[ITEM_PRICE],\n",
    "                \"Matched Master SKU\": None,\n",
    "                \"Matched Master Product\": None,\n",
    "                \"Match Probability\": best_match_scores[i],\n",
    "                \"Language\": query_row['detected_language']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_matching_with_timing(query_file, master_file, model, vectorizer , output_file=\"MatchedResults.xlsx\"):\n",
    "    \"\"\"Run matching with detailed timing information\"\"\"\n",
    "    print(\"Starting bilingual matching process...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = match_products_bilingual(query_file, master_file, model, vectorizer, output_file=output_file)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average time per product: {(total_time/len(results))*1000:.2f} ms\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer loaded successfully.\n",
      "Starting bilingual matching process...\n",
      "Starting bilingual matching process...\n",
      "Loaded 72 query products and 1000 master products\n",
      "Detecting languages and normalizing texts...\n",
      "Processing Arabic queries...\n",
      "Processing English queries...\n",
      "Matching completed in 3.37 seconds\n",
      "Average time per product: 46.80 ms\n",
      "\n",
      "Performance Summary:\n",
      "Total processing time: 3.37 seconds\n",
      "Average time per product: 46.82 ms\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Load the Pairwise XGBoost model\n",
    "model = xgb.Booster()\n",
    "model.load_model('ranking_product_matcher.json')\n",
    "\n",
    "# Load the TF-IDF vectorizer``\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully.\")\n",
    "\n",
    "results = run_matching_with_timing(\n",
    "    test,\n",
    "    master,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    output_file=\"MatchedResults_Ranking.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer loaded successfully.\n",
      "Starting bilingual matching process...\n",
      "Starting bilingual matching process...\n",
      "Loaded 72 query products and 1000 master products\n",
      "Detecting languages and normalizing texts...\n",
      "Processing Arabic queries...\n",
      "Processing English queries...\n",
      "Matching completed in 3.41 seconds\n",
      "Average time per product: 47.36 ms\n",
      "\n",
      "Performance Summary:\n",
      "Total processing time: 3.41 seconds\n",
      "Average time per product: 47.37 ms\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Load the Logistic XGBoost model\n",
    "model = xgb.Booster()\n",
    "model.load_model('logistic_boosting_model.json')\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully.\")\n",
    "results = run_matching_with_timing(\n",
    "    test,\n",
    "    master,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    output_file=\"MatchedResults_Logistic.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
